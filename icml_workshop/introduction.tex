\section{Introduction}
\label{sec:intro}

% Abstraction is useful.
Intelligent agents that leverage abstraction may reason about the salient features of their environment, while ignoring what is irrelevant. Consequently, agents that utilize abstraction are able to solve considerably more complex problems than they would be able to without the use of abstraction. 

% Planning and RL is hard.
For this reason abstraction is of considerable interest in planning and \ac{RL} work~\yrcite{li2006towards,abelhershko2016approx,baird1995residual,sutton1999policy,mnih2015human,konidaris2007building,dietterich2000hierarchical,sacerdoti1974planning,hauskrecht1998hierarchical,anand2015novel,parr1998reinforcement,sutton1999between,konidaris2015constructing,Jiang2015,andre2002state}. Planning and \ac{RL} problems tackle solving for optimal behavior in \acp{MDP}. 

A \ac{MDP} is a 5-tuple, $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$. $\mathcal{S}$ is a finite state space; $\mathcal{A}$ is a finite set of actions available to the agent; $\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the probability of an agent transitioning to state $s' \in \mathcal{S}$ after applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$; $\mathcal{R}(s,a)$ denotes the reward received by the agent for executing action $a$ in state $s$; $\gamma \in [0, 1]$ is a discount factor that determines how much the agent prefers future rewards over immediate rewards. We assume without loss of generality that the range of all reward functions is normalized to $[0,1]$. The solution to an \ac{MDP} is called a policy, denoted $\pi: \mathcal{S} \mapsto \mathcal{A}$. The objective of an agent is to solve for the policy that maximizes its expected discounted reward from any state, denoted $\pi^*$. 

 Although planning for optimal behavior in \acp{MDP} is known to be P-Complete in the size of the state and action space~\cite{papadimitriou1987complexity,littman1995complexity}, and many \ac{RL} algorithms for solving \acp{MDP} are known to require a number of samples only polynomial in the size of the state space~\cite{strehl2009reinforcement}, many state spaces are known to grown super-polynomially with the number of variables that characterize the domain \cite{abel2015goal} and robust action spaces are often large or even continuous~\cite{antos2008fitted,konidaris2014constructing}. Abstraction, then, promises an alluring avenue through which planning and \ac{RL} techniques may gain traction in complex domains. 

% Summarize related work.
\dnote{summary of related}
Lorem ipsum.

% We propose desiderata.
In this work, we characterize what it means for an abstraction to be effective in planning and \ac{RL}. In particular, we propose three desiderata that a theory of abstraction should satisfy. The first we call \textsc{Compressibility}, which stipulates that a theory on abstraction ought to achieve {\it compression}. The second we coin \textsc{Optimality}, which states that abstractions ought to maintain at least some notion of optimality. The last we term \textsc{Efficient-Computability} which states that an abstraction ought to be tractably computable.

% Outline: We summarize framework, propose extension and then ground out desiderata in framework.
Additionally, we summarize our previously proposed formal framework for state-abstraction methods, and demonstrate that the framework allows for careful formal analysis of temporally-extended abstractions. In particular the framework allows us to characterize what types of temporal abstraction do and do not satisfy \textsc{Compressibility}, \textsc{Optimality} and \textsc{Efficient-Computability}. Of special interest is proving what the necessary or sufficient conditions are for an option to be useful in planning or learning. \dnote{come back to dis sentence}
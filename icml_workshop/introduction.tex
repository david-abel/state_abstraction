\section{Introduction}
\label{sec:intro}

% Abstraction is useful.
Intelligent agents that leverage abstraction may reason about the salient features of their environment, while ignoring what is irrelevant. Consequently, agents that utilize abstraction are able to solve considerably more complex problems than they would be able to without the use of abstraction. 

% Planning and RL is hard.
For this reason abstraction is of considerable interest in planning and \ac{RL} work~\yrcite{li2006towards,abelhershko2016approx,baird1995residual,sutton1999policy,mnih2015human,konidaris2007building,dietterich2000hierarchical,sacerdoti1974planning,hauskrecht1998hierarchical,anand2015novel,parr1998reinforcement,sutton1999between,konidaris2015constructing,Jiang2015,andre2002state}. Planning and \ac{RL} formalize the sequential decision making problem as solving for optimal behavior in \acp{MDP}. 

An \ac{MDP} is a 5-tuple, $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$. $\mathcal{S}$ is a finite state space; $\mathcal{A}$ is a finite set of actions available to the agent; $\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the probability of an agent transitioning to state $s' \in \mathcal{S}$ after applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$; $\mathcal{R}(s,a)$ denotes the reward received by the agent for executing action $a$ in state $s$; $\gamma \in [0, 1]$ is a discount factor that determines how much the agent prefers future rewards over immediate rewards. We assume without loss of generality that the range of all reward functions is normalized to $[0,1]$. The solution to an \ac{MDP} is called a policy, denoted $\pi: \mathcal{S} \mapsto \mathcal{A}$. The objective of an agent is to solve for the policy that maximizes its expected discounted reward from any state, denoted $\pi^*$. 

 Although planning for optimal behavior in \acp{MDP} is known to be P-Complete in the size of the state and action space~\cite{papadimitriou1987complexity,littman1995complexity}, and many \ac{RL} algorithms for solving \acp{MDP} are known to require a number of samples only polynomial in the size of the state space~\cite{strehl2009reinforcement}, many state spaces are known to grown super-polynomially with the number of variables that characterize the domain \cite{abel2015goal} and robust action spaces are often large or even continuous~\cite{antos2008fitted,konidaris2014constructing}. Abstraction, then, promises an alluring avenue through which planning and \ac{RL} techniques may gain traction in complex domains. 

% Summarize related work.
Much has been done to mitigate the computational burden of large $\mathcal{S}$ and $\mathcal{A}$ in \acp{MDP}. A vast body of literature has investigated state abstraction methods, which seek to compress large state spaces into small ones. Additionally, function approximation has achieved a great deal of success in \ac{RL} for estimating the model, the value function, or the policy directly~\cite{mnih2015human,sutton1999policy,baird1995residual}. Lastly, hierarchical and skill based approaches have investigated temporal abstractions, such as options~\cite{sutton1999between}, which can serve as mechanisms for simplifying tasks~\cite{konidaris2015constructing}.

Notably, these methods are all evaluated under different criteria; some approaches are tested on different empirical domains, while others are shown to possess desirable theoretical properties. Furthermore, even when evaluated under similar criteria, the reliability and usefulness of the abstraction method is left unclear, as is the case with options. \cite{jong2008utility} demonstrate that options are not always useful, and can occasionally increase learning time. Conversely, other work have demonstrated that options can be useful in a variety of domains~\cite{sutton1999between}.

% We propose desiderata.

In this work, we characterize what it means for an abstraction to be effective in planning and \ac{RL}. The primary agenda of this characterization is to unify the goals of work in abstraction as it relates to \acp{MDP}.. To this end, we propose three desiderata that an abstraction ought to satisfy. The first we call \textsc{Compressibility}, which stipulates that an abstraction ought to achieve {\it compression}. The second we coin \textsc{Optimality}, which states that abstractions ought to maintain at least some notion of optimality. The last we term \textsc{Efficient-Computability} which states that an abstraction ought to be tractably computable. We summarize the state of current abstraction approaches in \ac{RL} and planning, and discuss a recent framework we developed as it relates to these desiderata.


% Outline: We summarize framework, propose extension and then ground out desiderata in framework.
%Additionally, we summarize our previously proposed formal framework for state-abstraction methods, and demonstrate that the framework allows for careful formal analysis of temporally-extended abstractions. In particular the framework allows us to characterize what types of temporal abstraction do and do not satisfy \textsc{Compressibility}, \textsc{Optimality} and \textsc{Efficient-Computability}. An open problem of special interest is to prove what the necessary or sufficient conditions are for an option to be useful in planning or learning. \dnote{come back to dis sentence}
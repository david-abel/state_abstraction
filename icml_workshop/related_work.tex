\section{Related Work}

Considerable work in planning and \ac{RL} has grappled with, either in the form of state aggregation (DEFINE?) or temporally extended actions (DEFINE?).



\subsection{Abstraction as State Aggregation}

\subsection{Approximate State Abstraction}

\citet{dean1997model} investigated partitioning an \ac{MDP}'s state space into $\varepsilon$-homogenous blocks, which are defined as clusters of states whose transition model and reward function are within $\varepsilon$ of each other. They develop an algorithm called Interval Value Iteration (IVI) that converges to the correct bounds on a family of abstract MDPs called Bounded \acp{MDP}. Several approaches build on the notion of $\varepsilon$-homogeneity. \citet{even2003approximate} analyzed different distance metrics used in the process of identifying $\varepsilon$-homogenous partitions. \citet{ortner2013adaptive} developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for $\mcT$ and $\mcR$ provided by UCRL~\cite{auer2009near}.

\dnote{We'll definitely need to discuss Bai, Srivasta, and Russell's new paper at IJCAI 2016.}

Many previous works have targeted the creation of algorithms that enable state abstraction for MDPs. \citet{andre2002state} investigated a method for state abstraction in hierarchical reinforcement learning leveraging a programming language called ALISP that promotes the notion of {\it safe} state abstraction. Agents programmed using ALISP can ignore irrelevant parts of the state, achieving abstractions that maintain optimality. 

\citet{dietterich2000hierarchical} developed MAXQ, a framework for composing tasks into an abstracted hierarchy where state aggregation can be applied. \citet{jong2005state} introduced a method called {\it policy-irrelevance} in which agents identify (online) which state variables may be safely abstracted away in a factored-state \ac{MDP}. 

For a more complete survey of algorithms that leverage state aggregation in past reinforcement-learning papers, see \citet{li2006towards}.

\subsection{Exact Abstraction Framework}

\citet{li2006towards} developed a framework for exact state abstraction in \acp{MDP}. In particular, the authors defined five classes of state-aggregation functions, inspired by existing methods for state aggregation in \acp{MDP}. We generalize two of these five classes, $\phi_{Q^*}$ and $\phi_{\text{model}}$, to the approximate abstraction case. Our generalizations are equivalent to theirs when exact criteria are used. Additionally, when exact criteria are used our bounds indicate that no value is lost, which is one of core results of \citet{li2006towards}.


\subsection{Abstraction as Temporal Extension}
Previous \textit{macro-action} or \textit{options}

\subsection{Abstraction as Function Approximation}

The use of function approximators for estimating the value function, model, or policy have seen extraordinary success in a variety of domains~\cite{mnih2015human,sutton1999policy,baird1995residual,stadie2015incentivizing,ormoneit2002kernel}. The use of such techniques achieves abstraction of a certain form - states that share some number of state variables receive similar signal from individual learning instances, thus generalizing knowledge far beyond an individual state. While some theoretical work has been done in this area~\cite{schoknecht2002optimality,thrun1993issues,parr2008analysis}, such analysis involves highly constraining assumptions (such as linearity or smoothness of the function class). Given that the goal of this work is to characterize effective abstractions by evaluating their relative satisfaction of each of our three desiderata, we stipulate that such analysis will prove illusive for these approaches.
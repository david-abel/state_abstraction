The combinatorial explosion that plagues planning and reinforcement learning algorithms can be reversed using abstraction. For instance, prohibitively difficult robotics task representations can be condensed so that solutions are tractably computable. In this work, we investigate a theoretical framework for approximate state abstraction that preserves near optimal behavior. Reinforcement learning agents using these abstractions may treat experiences that resemble each other as equivalent, and generalize knowledge to novel scenarios based on prior experiences. We present preliminary results and directions for future work.
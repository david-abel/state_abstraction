\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\input{../p.tex}
\title{Approximating the Optimal State Abstraction}
\author{}
\date{}                                           % Activate to display a given date or no date

% --- Note Commands ---
\newcommand\davenote[1]{\textcolor{blue}{Dave: #1}}

\begin{document}
\maketitle

% --- ABSTRACT ---
\begin{abstract}

\end{abstract}


% --- SECTION: Background ---
\newpage
\section{Background}

\subsection{The Problem}
\begin{mdframed}
\vspace{1mm}
Goal: planning and learning in large\footnote{TOTALLY MASSIVE} state spaces. \\
Proposed Strategy: abstract the problem to simplify.
\end{mdframed}

\vspace{4mm}
\elem{For example:}
Historically, we've relied on OO-MDPs a lot. There are other possible representations one might consider to collapse our representation of the real world. Objects seem like a reasonable choice given how object-oriented the human experience seems to be.

%For vision, other compressions make sense.
%For other sensor channels, different compressions make sense.
%For different worlds, different compressions make sense.

The main point is that compression helps, but we don't totally know how effective these things could be, and we don't know exactly which compressions to choose. In some sense, we're just picking compressions based on what we think might work.

Some questions to consider:
\begin{itemize}
\item Q: Is there a unified definition of state abstraction?
\item Q: How do different abstractions relate to one another?
\item Q: How can we select among abstraction schemes?
\item Q: How does the solution to the abstracted MDP relate to the original MDP?
\item Q: How should we be compressing? Does it change from domain to domain, task to task?
\item Q: Can we learn these abstractions? How much data do they require?
\end{itemize}

Immediate question: How should we compress? Any ideas? Does it change depending on the domain? The task? Are there other things we can use compression for?

So it turns out there have been a huge number of attempts at this:
\begin{itemize}
\item Dynamic Programming Aggregation
\item Stochastic DP with factored representations
\item Model reduction techniques
\item Model minimization...
\item Abstraction Selection...
\item State Abstraction Selection from...
\item Proto-value functions
\item Selecting the state-representation in reinforcement learning
\item Optimal regret bounds for selecting the...
\item RL with selective perception
\item SMDP homomorphisms: an algebraic approach.
\item Selecting near optimal approximate state...
\end{itemize}

% Subsection: Unified Theory Paper
\midline
\subsection{Unified Theory Paper}
Michael, Tom Walsh, Lihong Li wrote a paper called ``Towards a Unified Theory of State Abstraction for MDPs''~\cite{li2006towards} that surveys these approaches, and puts them into a single framework.

The high level is that there are roughly 5 types of abstractions:
\begin{enumerate}
% Model Irrelevance
\item Model-irrelevance: $\phi_{\text{model}}$ :
\begin{multline*}
\forall_s \forall_a \phi_{\text{model}}(s_1) = \phi_{\text{model}}(s_2) \rightarrow \mathcal{R}(s_1,a) = \mathcal{R}(s_2,a)\ \wedge \\
\sum_{s' \in \phi_{\text{model}}^{-1}(s)} \Pr(s' \mid s_1, a) = \sum_{s' \in \phi_{\text{model}}^{-1}(s)} \Pr(s' \mid s_2, a)
\end{multline*}

% Q^\pi Irrelevance
\item $Q^\pi$-irrelevance: $\phi_{Q^\pi}$ :
\begin{equation*}
\forall_\pi \phi_{Q^\pi}(s_1) = \phi_{Q^\pi}(s_2) \rightarrow  \forall_a Q^\pi(s_1,a) = Q^\pi(s_2,a)
\end{equation*}

% Q^* Irrelevance
\item $Q^*$-irrelevance: $\phi_{Q^*}$ :
\begin{equation*}
\phi_Q^*(s_1) = \phi_Q^*(s_2) \rightarrow \forall_a  Q^*(s_1,a) = Q^*(s_2,a)
\end{equation*}

% a^* Irrelevance
\item $a^*$-irrelevance: $\phi_{a^*}$:
\begin{equation*}
\phi_{a^*}(s_1) = \phi_{a^*}(s_2) \rightarrow Q^*(s_1,a^*) = \max_a Q^*(s_1,a) = \max_a Q^*(s_2,a) = Q^*(s_2, a^*)
\end{equation*}

% \pi^* Irrelevance
\item $\pi^*$-irrelevance: $\phi_{\pi^*}$:
\begin{equation*}
\phi_{\pi^*}(s_1) = \phi_{\pi^*}(s_2) \rightarrow \left( Q^*(s_1, a^*) = \max_a Q^*(s_1,a) \wedge Q^*(s_2,a^*) = \max_a Q^*(s_2,a) \right)
\end{equation*}

\end{enumerate}

One note: I haven't thought about this too much, but I'm pretty sure that throwing in abstracted actions like Options fits into this framework neatly -- suppose we're in an SMDP and that an option is just an action. Then we can abstract the state space based on where subgoals are/aren't satisfied. Seems like the right sort of result.

Other results from the paper:

\thm{Theorem}{3}{With abstractions $\phi_{\text{model}}$, $\phi_{Q^\pi}$, $\phi_{Q^*}$, and $\phi_{a^*}$, the optimal abstract policy $\bar{\pi}^*$ is optimal in the ground MDP.}

\thm{Theorem}{4.1}{Q-Learning with abstractions $\phi_{\text{model}}$, $\phi_{Q^\pi}$, and $\phi_{Q^*}$, converges to the optimal state-action value function in the ground MDP}

\thm{Theorem}{4.2}{Q-Learning with abstraction $\phi_{a^*}$ does not necessarily converge.}

\thm{Theorem}{4.3}{Q-Learning with abstraction $\phi_{\pi^*}$ can converge to an action-value function whose greedy-policy is suboptimal in the ground MDP.}

In a follow up paper, they have a distribution on MDPs, sample some training MDPs to infer the optimal state abstraction, and use it to solve a test MDP from the same distribution.



% --- SECTION: Motivation ---
\newpage
\section{Motivation}

\elem{Basic Point:} State abstraction for planning, learning, possibly bandits could be EPIC.

\elem{Result 1:} Arbitrary reduction in Sample Complexity for a particular MDP.

\elem{Result 2:} More general result about sample complexity reduction for MDPs of a certain type


The Question: Assuming a given MDP has property set X, can we say anything about the possible reduction in sample complexity using a state abstraction function.


Possible other Result: Exploration.



% --- SECTION: Elephants In the Room ---
\section{Elephants in the Room}

\begin{enumerate}
\item Finding states with {\it exactly} the same Q values, or optimal action and action value, or same model, is rare!
\item Can't capture temporal abstraction.
\end{enumerate}

Proposal: Resolve (1) and (2), so that the nice results from Section 2 can be realized. The next two sections talk about how we might go about doing that.

\midline

% --- SECTION: New Proposal ---
\newpage
\section{New Proposal 1: Approximate Abstraction}

For each of the following four cases:

% Model Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_{s,a} : T(s \mid s_1, a) = T(s \mid s_2, a) \wedge \forall_a : R(s_1,a) = R(s_2,a)
\end{equation}

% Transition Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_{s,a} : T(s \mid s_i, a) = T(s \mid s_j, a)
\end{equation}

% Optimal Q Function Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_a : Q^*(s_i, a) = Q^*(s_j, a)
\end{equation}

% Optimal action and Q value Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \argmax_a Q^*(s_i, a) = \argmax_a Q^*(s_j, a) \wedge \max_a Q^*(s_i, a) = \max_a Q^*(s_j, a)
\end{equation}

% Possible I-Learning?

% Ordering of Q values?

\elem{Our strategy:} relax the equality condition, instead consider the approximate case, e.g.:
% Model Similarity
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_{s,a} : |T(s \mid s_i, a) - T(s \mid s_j, a)| \leq \epsilon
\end{equation}

Sort of like the Simulation Lemma from $E^3$.

\subsection{The Question:} Suppose we're given the optimal policy in the abstract MDP under one of the above approximate abstractions, e.g. $\pi^*_{\phi_{T,\epsilon}}$. What can be said about the potential optimality of this policy in the original ground MDP? (for each of the four approximate abstractions).

\subsection{Followup Question:} Why {\it won't} this work for other types of abstractions?
\begin{itemize}
\item Just using state variables...
\item Just Reward function at a state...
\item Just Value of state...
\item Any combination of these...
\end{itemize}

Results about which abstractions {\it don't} work. Note: Already have counter examples for the above 3, would be nice to get more general results.

The result is that we identify which criteria are at the core of making state abstraction {\it useful}.



% --- SECTION: Results! ---
\newpage
\section{Results}

\thm{Lemma}{1}{Given two MDPs, $M_1$ and $M_2$ which differ only by $T$ and $R$, if $\forall_{s,a} |T_1(s,a,\cdot)-T_2(s,a,\cdot)| \leq 2\beta$ and $\forall_{s,a} |R_1(s,a)-R_2(s,a)|\leq \alpha$ then for some fixed policy $\pi$ then:
\begin{equation}
\forall_{s,a} |Q_1^\pi(s,a)-Q_2^\pi(s,a)| \leq \frac{\alpha \textsc{Vmax}}{C(1-\gamma)} = \epsilon
\end{equation} (Simulation Lemma)}

Call the ground truth MDP $M_G$, the abstract MDP $M_A=\phi(M_G)$.\footnote{Sloppy notation -- really $\phi$ is only applied to $S$.} Suppose that the agent plans in $M_A$. We would like to bound how well it is \textit{really} performing in $M_G$ based on how it performs in $M_A$. 

Consider the decompressed ground state space of $M_A$, $M_{G'} = \phi^{-1}(\phi(M_G))$.

\thm{Lemma}{2}{Under the model approximate abstraction scheme, $\phi_{model}$, the Q values under any policy in $M_A$ will be within $\epsilon$ of Q values under the same policy in $M_G$.}

\elem{Proof:} Recall our definition of model approximate abstraction:
\begin{equation}
\forall_{s_1,s_2} \phi(s_1) = \phi(s_2) \rightarrow \forall_{s,a} | T(s,a,s_1) - T(s,a,s_2) | \leq 2\beta \wedge \forall_a |R(s_1,a) - R(s_2,a)| \leq \alpha
\label{eq:approx_model_abs}
\end{equation}

Sim Lemma says we need T and R to be within $2\beta = \alpha$.

We have our ground MDP: $M_G$, abstract MDP, $M_A$, and noisy ground MDP, $M_{G'}$.

Using a given state abstraction function $\phi$ subject to Equation~\ref{eq:approx_model_abs}, the difference between $T_G$ and $T_{G'}$.

Sim lemma goes through (cause same for $R$).

\thm{Lemma}{3}{Under the optimal Q function approximate abstraction, $\forall_{s,a} : |Q_A^*(s,a) - Q_G^*(s,a)| \leq \epsilon$}

Since for each state, the most you can screw up is by $\epsilon$, since if you don't take $\argmax_a Q^*(s,a)$, then one of the other actions Q functions must have jumped up by at most $\epsilon$, but it's fine because then you're doing no worse then $\epsilon$. It can't have jump mored, because otherwise you wouldn't have compressed on it.

% Note: this might be too tight. A looser bound is $\epsilon D$, where $D$ is the diameter of the MDP, since each state you can incur at most $\epsilon$ error? This seems wrong too, it's not $D$ necessarily? It's more the number of actions executed during planning/learning. Does that seem right?
% Doesn't it depend on the policy? Like this assumes we're working under a greedy Q policy. Suppose we're using an {\it arbitrary} policy... 

% --- SECTION: Temporal Abstraction ---
\newpage
\section{New Proposal 2: Temporal Abstraction}

In light of the above negative results (regarding which abstractions are useful), that state abstraction needs to be defined with respect to some sort of properties about actions, suppose a set of temporally extended actions are included in the action set.

Now, abstractions become temporally extended.

What sort of thing do we want to prove here? Abstractions become temporal if actions are temporally extended? Without temporal extension, forces $T(s,a,s')$ and $R(s,a)$ to do weird things?

Q: Can we treat temporally extended actions in a general enough way to prove things about all possible versions of TEAs, or do we need to do separate proofs for Options/Macroactions?


% --- SECTION: Other Open Questions + Ideas ---
\newpage
\section{Other Open Questions + Ideas}


% Subsection: Things you should not compress on.
\subsection{Result 3: Collapsing on just {\it state variables}, or {\it reward}, or {\it value} is insufficient to preserve any kind of optimality (including $\epsilon$-optimality.}

\elem{IDEA:} How about probability matching? I.e. what if we do:

\begin{equation}
\forall_{s_1,s_2} : \phi(s_1) = \phi(s_2) \rightarrow KL(\Delta_1 || \Delta_2) \leq \epsilon
\end{equation}

Where $\Delta_i$ is a softmax over Q values at state $s_i$, meaning:
\begin{equation}
\Delta_i = \frac{e^{Q(s,a)}}{\sum_b Q(s,b)}
\end{equation}



% --- SECTION: Game Plan ---
\newpage{Game Plan}

First, get results about the approximate state abstraction for each of the 4 under consideration.

Second, investigate temporally extended actions w/ abstraction.

Third, wrap up low hanging fruit results, investigate more general versions of these claims (beyond $\exists MDP s.t. \ldots$.).


% --- SECTION: Conclusion ---
\newpage
\section{Conclusion}

Future Work:
\begin{itemize}
\item Learning $\phi$
\item Connection to AMDPs (e.g. consider $\Omega : \langle S, A, R, T, \gamma \rangle \mapsto \langle S', A', R', T', \gamma' \rangle$
\item Connection to teaching.
\begin{itemize}
\item AMDPs seem well suited to hierarchical teaching Carl-style.
\item Problem: once solved for optimal policy in AMDP already solved in low level state. Probably want to set up teaching to avoid this.
\end{itemize}
\end{itemize}



% --- BIBLIOGRAPHY ---
\newpage
\bibliography{sca_bib}

\end{document}

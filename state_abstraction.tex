\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\input{p.tex}
\title{Approximating the Optimal State Abstraction}
\author{}
\date{}                                           % Activate to display a given date or no date

% --- Note Commands ---
\newcommand\davenote[1]{\textcolor{blue}{Dave: #1}}

\begin{document}
\maketitle

% --- ABSTRACT ---
\begin{abstract}

\end{abstract}


% --- SECTION: Background ---
\newpage
\section{Background}

\subsection{The Problem}
\begin{mdframed}
\vspace{1mm}
Goal: planning and learning in large\footnote{TOTALLY MASSIVE} state spaces. \\
Proposed Strategy: abstract the problem to simplify.
\end{mdframed}

\vspace{4mm}
\elem{For example:}
Historically, we've relied on OO-MDPs a lot. There are other possible representations one might consider to collapse our representation of the real world. Objects seem like a reasonable choice given how object-oriented the human experience seems to be.

%For vision, other compressions make sense.
%For other sensor channels, different compressions make sense.
%For different worlds, different compressions make sense.

The main point is that compression helps, but we don't totally know how effective these things could be, and we don't know exactly which compressions to choose. In some sense, we're just picking compressions based on what we think might work.

Some questions to consider:
\begin{itemize}
\item Q: Is there a unified definition of state abstraction?
\item Q: How do different abstractions relate to one another?
\item Q: How can we select among abstraction schemes?
\item Q: How does the solution to the abstracted MDP relate to the original MDP?
\item Q: How should we be compressing? Does it change from domain to domain, task to task?
\item Q: Can we learn these abstractions? How much data do they require?
\end{itemize}

Immediate question: How should we compress? Any ideas? Does it change depending on the domain? The task? Are there other things we can use compression for?

So it turns out there have been a huge number of attempts at this:
\begin{itemize}
\item Dynamic Programming Aggregation
\item Stochastic DP with factored representations
\item Model reduction techniques
\item Model minimization...
\item Abstraction Selection...
\item State Abstraction Selection from...
\item Proto-value functions
\item Selecting the state-representation in reinforcement learning
\item Optimal regret bounds for selecting the...
\item RL with selective perception
\item SMDP homomorphisms: an algebraic approach.
\item Selecting near optimal approximate state...
\end{itemize}

% Subsection: Unified Theory Paper
\midline
\subsection{Unified Theory Paper}
Michael, Tom Walsh, Lihong Li wrote a paper called ``Towards a Unified Theory of State Abstraction for MDPs''~\cite{li2006towards} that surveys these approaches, and puts them into a single framework.

The high level is that there are roughly 5 types of abstractions:
\begin{enumerate}
% Model Irrelevance
\item Model-irrelevance: $\phi_{\text{model}}$ :
\begin{multline*}
\forall_s \forall_a \phi_{\text{model}}(s_1) = \phi_{\text{model}}(s_2) \rightarrow \mathcal{R}(s_1,a) = \mathcal{R}(s_2,a)\ \wedge \\
\sum_{s' \in \phi_{\text{model}}^{-1}(s)} \Pr(s' \mid s_1, a) = \sum_{s' \in \phi_{\text{model}}^{-1}(s)} \Pr(s' \mid s_2, a)
\end{multline*}

% Q^\pi Irrelevance
\item $Q^\pi$-irrelevance: $\phi_{Q^\pi}$ :
\begin{equation*}
\forall_\pi \phi_{Q^\pi}(s_1) = \phi_{Q^\pi}(s_2) \rightarrow  \forall_a Q^\pi(s_1,a) = Q^\pi(s_2,a)
\end{equation*}

% Q^* Irrelevance
\item $Q^*$-irrelevance: $\phi_{Q^*}$ :
\begin{equation*}
\phi_Q^*(s_1) = \phi_Q^*(s_2) \rightarrow \forall_a  Q^*(s_1,a) = Q^*(s_2,a)
\end{equation*}

% a^* Irrelevance
\item $a^*$-irrelevance: $\phi_{a^*}$:
\begin{equation*}
\phi_{a^*}(s_1) = \phi_a^*(s_2) \rightarrow Q^*(s_1,a^*) = \max_a Q^*(s_1,a) = \max_a Q^*(s_2,a) = Q^*(s_2, a)
\end{equation*}

% \pi^* Irrelevance
\item $\pi^*$-irrelevance: $\phi_{\pi^*}$:
\begin{equation*}
\phi_{\pi^*}(s_1) = \phi_{\pi^*}(s_2) \rightarrow \left( Q^*(s_1, a^*) = \max_a Q^*(s_1,a) \wedge Q^*(s_2,a^*) = \max_a Q^*(s_2,a) \right)
\end{equation*}

\end{enumerate}

One note: I haven't thought about this too much, but I'm pretty sure that throwing in abstracted actions like Options fits into this framework neatly -- suppose we're in an SMDP and that an option is just an action. Then we can abstract the state space based on where subgoals are/aren't satisfied. Seems like the right sort of result.

Other results from the paper:

\thm{Theorem}{3}{With abstractions $\phi_{\text{model}}$, $\phi_{Q^\pi}$, $\phi_{Q^*}$, and $\phi_{a^*}$, the optimal abstract policy $\bar{\pi}^*$ is optimal in the ground MDP.}

\thm{Theorem}{4.1}{Q-Learning with abstractions $\phi_{\text{model}}$, $\phi_{Q^\pi}$, and $\phi_{Q^*}$, converges to the optimal state-action value function in the ground MDP}

\thm{Theorem}{4.2}{Q-Learning with abstraction $\phi_{a^*}$ does not necessarily converge.}

\thm{Theorem}{4.3}{Q-Learning with abstraction $\phi_{\pi^*}$ can converge to an action-value function whose greedy-policy is suboptimal in the ground MDP.}

In a follow up paper, they have a distribution on MDPs, sample some training MDPs to infer the optimal state abstraction, and use it to solve a test MDP from the same distribution.



% --- SECTION: Motivation ---
\newpage
\section{Motivation}

\elem{Basic Point:} State abstraction for planning, learning, possibly bandits could be EPIC.

% Subsection: Arbitrary reduction in Sample Complexity for a specific MDP.
\subsection{Result 1: Arbitrary reduction in Sample Complexity for a particular MDP.}

% Subsection: More general SC reduction result.
\subsection{Result 2: More general result about sample complexity reduction for MDPs of a certain type}

Q: Assuming a given MDP has property set X, can we say anything about the possible reduction in sample complexity using a state abstraction function.


% Maybe something about exploration.


% Subsection: Things you should not compress on.
\subsection{Result 3: Collapsing on just {\it state variables}, or {\it reward}, or {\it other?} is insufficient to preserve any kind of optimality (including $\epsilon$-optimality.}



% --- SECTION: Elephants In the Room ---
\section{Elephants in the Room}

\begin{itemize}
\item Finding states with {\it exactly} the same Q values, or optimal action and action value, is rare!
\item Can't capture temporal abstraction.
\end{itemize}

\midline

% --- SECTION: New Proposal ---
\newpage
\section{New Proposal: Approximate Abstraction}


For each of the following three cases:
% Model Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_{s,a} : T(s \mid s_i, a) = T(s \mid s_j, a)
\end{equation}

% Optimal Q Function Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_a : Q^*(s_i, a) = Q^*(s_j, a)
\end{equation}

% Optimal action and value Equivalence
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \argmax_a Q^*(s_i, a) = \argmax_a Q^*(s_j, a) \wedge \max_a Q^*(s_i, a) = \max_a Q^*(s_j, a)
\end{equation}

Relax the equality condition, instead consider the approximate case, e.g.:
% Model Similarity
\begin{equation}
 \phi(s_1) = \phi(s_2) \rightarrow \forall_{s,a} : |T(s \mid s_i, a) - T(s \mid s_j, a)| \leq \epsilon
\end{equation}

\elem{The Question:} Can anything be said about the optimality of the policy $\pi_{\phi_{T,\epsilon}}$ in the ground MDP? Are we within some factor of $\epsilon$ away from being optimal?


% --- SECTION: Temporal Abstraction ---
\newpage
\section{New Proposal: Temporal Abstraction}

In light of Result 3, that state abstraction needs to be defined with respect to some sort of properties about actions, suppose a set of temporally extended actions are included in the action set.

Now, abstractions become temporally extended.

Should be some nice results there.




% --- SECTION: Conclusion ---
\newpage
\section{Conclusion}

Future Work:
\begin{itemize}
\item Learning $\phi$
\item Connection to AMDPs (e.g. consider $\Omega : \langle S, A, R, T, \gamma \rangle \mapsto \langle S', A', R', T', \gamma' \rangle$
\item Connection to teaching.
\begin{itemize}
\item AMDPs seem well suited to hierarchical teaching Carl-style.
\item Problem: once solved for optimal policy in AMDP already solved in low level state. Probably want to set up teaching to avoid this.
\end{itemize}
\end{itemize}



% --- BIBLIOGRAPHY ---
\newpage
\bibliography{sca_bib}

\end{document}

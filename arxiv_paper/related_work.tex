\section{Related Work}

Several other projects have addressed similar topics.

\subsection{Approximate State Abstraction}
\citet{dean1997model} leverage the notion of {\it bisimulation} to investigate partitioning an \ac{MDP}'s state space into clusters of states whose transition model and reward function are within $\varepsilon$ of each other. They develop an algorithm called Interval Value Iteration (IVI) that converges to the correct bounds on a family of abstract MDPs called Bounded \acp{MDP}.

Several approaches build on \citet{dean1997model}. \citet{ferns2004metrics,ferns2006methods} investigated state similarity metrics for \acp{MDP}; they bounded the value difference of ground states and abstract states for several bisimulation metrics that induce an abstract MDP. This differs from our work which develops a theory of abstraction that bounds the suboptimality of applying the optimal policy of an abstract MDP to its ground MDP, covering four types of state abstraction, one of which closely parallels bisimulation. \citet{even2003approximate} analyzed different distance metrics used in identifying state space partitions subject to $\varepsilon$-similarity, also providing value bounds (their Lemma 4) for $\eps$-homogeneity subject to the $L_\infty$ norm, which parallels our Claim 2. \citet{ortner2013adaptive} developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for $\mcT$ and $\mcR$ provided by UCRL~\cite{auer2009near}. \citet{Hostetler2014} apply state abstraction to the planning problem. Specifically, they investigate state abstraction for Monte Carlo Tree Search and expectimax search, giving value bounds of applying the optimal abstract action in the ground tree(s), similarly to our setting.~\citet{jiang2014improving} analyze a similar setting, applying abstractions to the Upper Confidence Bound applied to Trees algorithm, introduced by~\citet{kocsis2006bandit}.

\citet{hutter2016extreme,hutter2014extreme} investigates state aggregation beyond the MDP setting. Hutter presents a variety of results for aggregation functions in reinforcement learning. Most relevant to our investigation is Hutter's Theorem 8, which illustrates properties of aggregating states based on similar $Q$ values. Hutter's Theorem part (a) parallels our Claim: both bound the value difference between ground and abstraction states, and part (b) is analogous to our Lemma 1: both bound the value difference of applying the optimal abstraction policy in the ground, and part (c) is a repetition of the comment given by~\citet{li2006towards} that $Q^*$ abstractions preserve the optimal value function. For Lemma 1, our proof strategies differ from Hutter's, but the result is the same.

~\citet{mandelefficient} advance Bayesian aggregation in RL to define Thompson Clustering for Reinforcement Learning (TCRL), an extension of which achieves near-optimal Bayesian regret bounds. ~\citet{Jiang2015} analyze the problem of choosing between two candidate abstractions. They develop an algorithm based on statistical tests that trades of the approximation error with the estimation error of the two abstractions, yielding a loss bound on the quality of the chosen policy.

\subsection{Specific Abstraction Algorithms}
Many previous works have targeted the creation of algorithms that enable state abstraction for MDPs. \citet{andre2002state} investigated a method for state abstraction in hierarchical reinforcement learning leveraging a programming language called ALISP that promotes the notion of {\it safe} state abstraction. Agents programmed using ALISP can ignore irrelevant parts of the state, achieving abstractions that maintain optimality. \citet{dietterich2000hierarchical} developed MAXQ, a framework for composing tasks into an abstracted hierarchy where state aggregation can be applied.~\citet{bakker2004hierarchical} also target hierarchical abstraction, focusing on subgoal discovery. \citet{jong2005state} introduced a method called {\it policy-irrelevance} in which agents identify (online) which state variables may be safely abstracted away in a factored-state \ac{MDP}.~\citet{Dayan1993} develop ``Feudal Reinforcement Learning" which presents an early form of hierarchical RL that restructures $Q$ Learning to manage the decomposition of a task into subtasks. For a more complete survey of algorithms that leverage state abstraction in past reinforcement-learning papers, see \citet{li2006towards}, and for a survey of early works on hierarchical reinforcement learning, see \citet{barto2003recent}.


\subsection{Exact Abstraction Framework}

\citet{li2006towards} developed a framework for exact state abstraction in \acp{MDP}. In particular, the authors defined five types of state-aggregation functions, inspired by existing methods for state aggregation in \acp{MDP}. We generalize two of these five types, $\phi_{Q^*}$ and $\phi_{\text{model}}$, to the approximate abstraction case. Our generalizations are equivalent to theirs when exact criteria are used (i.e. $\varepsilon = 0$). Additionally, when exact criteria are used our bounds indicate that no value is lost, which is one of core results of \citet{li2006towards}.~\citet{walsh2006transferring} build on the framework they previously developed by showing empirically how to transfer abstractions between structurally related MDPs.

%zzz you shouldn't have $\phi_{model}$ because the word model will be typeset as the product of five variables. Some math mode magic is needed.

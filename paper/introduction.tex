\section{Introduction}
\label{sec:intro}

Abstraction plays a fundamental role in learning. Through abstraction, intelligent agents may reason about only the salient features of their environment while ignoring what is irrelevant. Consequently, agents are able to solve considerably more complex problems than they would be able to without the use of abstraction. However, \textit{exact abstractions}, which treat only fully-identical situations as equivalent, require complete knowledge that is often computationally intractable to obtain. Furthermore, often no two situations are identical, so exact abstractions are often ineffective. To overcome these issues, we investigate \textit{approximate abstractions} that enable agents to treat sufficiently similar situations as identical. This work characterizes the impact of equating ``sufficiently similar'' states in the context of planning and \ac{RL} in \acp{MDP}. The remainder of our introduction contextualizes these intuitions in \acp{MDP}.

% Abstraction is a good idea b/c MDP has poly comp comp and samp comp in S, but S grows fast.
Solving for optimal behavior in \acp{MDP} in a planning setting is known to be P-Complete in the size of the state space~\cite{papadimitriou1987complexity,littman1995complexity}. Similarly, many \ac{RL} algorithms for solving \acp{MDP} are known to require a number of samples polynomial in the size of the state space~\cite{strehl2009reinforcement}. Although polynomial runtime or sample complexity may seem like a reasonable constraint, the size of the state space of an \ac{MDP} grows super-polynomially with the number of variables that characterize the domain, a result of Bellman's curse of dimensionality. Thus, solutions polynomial in state space size are often ineffective for sufficiently complex tasks. \dnote{Would prefer a different example} For instance, a robot involved in a pick-and-place task might be able to employ planning algorithms to solve for how to manipulate some objects into a desired configuration in time polynomial in the number of states, but the number of states it must consider grows exponentially with the number of objects it is working with \enote{Cite Dabe Icaps}.

% Existing work on state abstraction.
Thus, a key research agenda for planning and \ac{RL} is leveraging abstraction to reduce large state spaces~\cite{andre2002state,jong2005state,dietterich2000hierarchical,Bean2011}. This agenda has given rise to methods that reduce \textit{ground} \acp{MDP} with large state spaces to \textit{abstract} MDPs with smaller state spaces by aggregating states according to some notion of equality or similarity. In the context of \acp{MDP}, we understand exact abstractions as those that aggregate states with equal values of particular quantities, for example, optimal $Q$-values. Existing work has characterized how exact abstractions can fully maintain optimality in \acp{MDP}~\cite{li2006towards,dean1997modelmin}. 

% Thesis: Approximate abstraction for three reasons.
The thesis of this work is that performing approximate abstraction in \acp{MDP} by relaxing the state-aggregation criteria from equality of quantities to $\varepsilon$-closeness maintains bounded error in the resulting behavior while offering three benefits. First, approximate abstractions employ the sort of knowledge that we expect a planning or learning algorithm to compute without fully solving the \ac{MDP}. In contrast, exact abstractions often require solving for optimal behavior, thereby defeating the purpose of abstraction. Second, because of their relaxed criteria, approximate abstractions can achieve greater degrees of compression than exact abstractions. This difference is particularly important in environments where no two states are identical. \dnote{note this in empirical results discussion, that random is sort of like a natural environment and when eps is 0, no compression} Third, because the state-aggregation criteria are relaxed to near equality, approximate abstractions are able to tune the aggressiveness of abstraction by adjusting what they consider sufficiently similar states. 

% Summary of the four abstractions.
We support this thesis by describing four different classes of approximate abstraction functions that preserve near-optimal behavior. Each function family introduced aggregates states based on different approximate criteria: $\epQ$, on similar optimal $Q$-values, $\epM$, on similarity of rewards and transitions, $\epB$, on similarity of a Boltzmann distribution over optimal $Q$-values, and $\epMu$, on similarity of a multinomial distribution over optimal $Q$-values. Furthermore, we empirically demonstrate the relationship between the degree of compression and error incurred on a variety of \acp{MDP}.

% Summary	
%\dnote{If we keep this, add section numbers, make it match the actual sections b/c we changed their order}
%This paper is organized as follows. We first introduce the necessary terminology and background of \acp{MDP} and state abstraction. We then survey existing types of state abstraction as applied to sequential decision making. The following section introduces our primary result; bounds on the error guaranteed by four classes of approximate state abstraction. We then discuss experiments in which we apply one class of approximate abstraction to a variety of different tasks, visualizing the abstractions and showing the relationship between degree of compression and error incurred.

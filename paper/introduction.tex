\section{Introduction}
Abstraction plays a fundamental role in learning. Through abstraction, intelligent agents may reason about only the salient features of their environment while ignoring what is irrelevant. Consequently, agents are able to solve considerably more complex problems than they would be able to without the use of abstraction. However, perfect abstractions, which treat fully-identical situations alike, require complete knowledge that is often computationally intractable. Furthermore, in natural environments, no two states are identical, so perfect abstraction is ineffective. To overcome these issues we investigate approximate abstractions that enable agents to treat sufficiently similar states as identical. This work characterizes the impact of combining ``sufficiently similar'' states in the context of planning and \ac{RL} in \acp{MDP}. 

\dnote{Want a clause, sentence, prhase, paragraph, that transitions us from the super high level biz in the first paragraph to the second paragraph}
Abstraction in the context of planning and \ac{RL} in an \ac{MDP} takes the form of state aggregation. \dnote{rephrase and add a sentence or two that gets us to paragraph two}

% Abstraction is a good idea b/c MDP has poly comp comp and samp comp in S, but S grows fast.
Solving for optimal behavior in \acp{MDP} in a planning setting is known to be P-Complete in the size of the state space~\cite{papadimitriou1987complexity,littman1995complexity}. Similarly, many \ac{RL} algorithms for solving \acp{MDP} are known to require a number of samples polynomial in the size of the state space~\cite{strehl2009reinforcement}. Although polynomial runtime or sample complexity may seem like a reasonable constraint, due to Bellman's curse of dimensionality, the size of the state space of an \ac{MDP} grows super-polynomially with the number of variables that characterize the domain. Thus, bounds polynomial in state space size often translate to intractable problems for sufficiently complicated tasks. \dnote{Would prefer a different example} For instance, a robot involved in a pick and place task might be able to employ planning algorithms to solve for how to manipulate some objects into a desired configuration in time polynomial in the number of states, but the number of states it must consider to do so grows exponentially with the number of objects it plans over \enote{Cite Dabe Icaps}.

% Existing work on state abstraction.
Thus, a key research agenda for planning and \ac{RL} is leveraging abstraction to reduce large state spaces~\cite{andre2002state,jong2005state,dietterich2000hierarchical,Bean2011}. These methods are characterized by reducing \textit{ground} MDPs with large state spaces to \textit{abstract} MDPs with smaller state spaces by aggregating states according to some notion of equality or similarity. Existing work has characterized how aggregation of states with equal values of particular quantities fully maintains optimality~\cite{li2006towards,dean1997modelmin}. However, exactly solving for these quantities is itself computationally taxing and often as difficult as solving for optimal behavior in the ground \ac{MDP}, thereby defeating the purpose of abstraction.

% Thesis: Approximate abstraction for three reasons.
The thesis of this work is that aggregation of states on the basis of various approximate criteria incur only bounded error in the resulting policy. Relaxing the aggregation criteria from equality of quantities to $\epsilon$-closeness offers three benefits. First, state aggregation algorithms that satisfy these criteria employ the sort of knowledge that we expect a planning or learning algorithm to approximate without fully solving the \ac{MDP}. Second, because of their relaxed criteria, approximate abstractions can achieve greater degrees of compression than perfect abstractions. This is particularly important in natural environments where no two states are identical. \dnote{note this in empirical results discussion, that random is sort of like a natural environment and when eps is 0, no compression} Third, because the state aggregation criteria are relaxed to approximate equality, they are able to tune the aggressiveness of abstraction by adjusting what they consider sufficiently similar states.


% Summary	
This paper is organized as follows. We first introduce the necessary terminology and background of \ac{RL} and state abstraction. We then survey existing types of state abstraction as applied to sequential decision making. The following section introduces our primary result; bounds on the error guaranteed by four classes of approximate state abstraction. We then discuss experiments in which we apply one class of approximate abstraction to a variety of different tasks, visualizing the abstractions and showing the relationship between degree of compression and error incurred.

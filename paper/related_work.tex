\section{Related Work}

Several other projects have addressed similar topics.

\subsection{Approximate State Abstraction}
\citep*{dean1997model} investigated partitioning an \ac{MDP}'s state space into $\epsilon$-homogenous blocks, which are defined as clusters of states whose transition model and reward function are within $\epsilon$ of each other. They develop an algorithm called Interval Value Iteration (IVI) that converges to the correct bounds on a family of abstracted MDPs called a Bounded MDP. This method of abstraction is effectively characterized as a function belonging to the approximate class $\phi_{model}$.
Several approaches build on the notion of $\epsilon$-homogeneity. \citep*{even2003approximate} analyzed different distance metrics used in the process of identifying $\epsilon$-homogenous partitions. \citep*{ortner2013adaptive} developed an algorithm for learning partitions in an online setting by taking advantage of the confidence bounds for $T$ and $gR$ provided by UCRL~\cite{auer2009near}.

zzz what's $gR$?

\subsection{Specific Abstraction Algorithms}
Many previous works have targeted the creation of algorithms that enable state abstraction for MDPs. \citep*{andre2002state} investigated a method for state abstraction in hierarchical reinforcement learning leveraging a programming language called ALISP that promotes the notion of {\it safe} state abstraction. Agents programmed using ALISP can ignore irrelevant parts of the state, achieving abstractions that maintain optimality. \citep{dietterich2000hierarchical} developed MAXQ, a framework for composing tasks into an abstracted hierarchy where state aggregation can be applied. \citep*{jong2005state} introduced a method called {\it policy-irrelevance} in which agents identify (online) which state variables may be safely abstracted away in a factored-state \ac{MDP}. The agent will learn to ignore state variables that don't impact optimal behavior, making this method a version of the approximate $\phi_{a^*}$ work zzz pointer??. For a more complete survey of algorithms that leverage state abstraction in past reinforcement-learning papers, see \citep{li2006towards}.

\subsection{Exact Abstraction Framework}

\citep*{li2006towards} developed a framework for exact state abstraction in \acp{MDP}. In particular, the authors defined five classes of state-aggregation functions, inspired by existing methods for state aggregation in \acp{MDP}. We generalize two of these five classes, $\phi_{Q^*}$ and $\phi_{model}$, to the approximate abstraction case. Note that when $\epsilon=0$ our generalizations are equivalent to the corresponding exact methods. Additionally, when $\epsilon=0$ our bounds indicate that no value is lost, which is one of core results of \citep{li2006towards}.

zzz you shouldn't have $\phi_{model}$ because the word model will be typeset as the product of five variables. Some math mode magic is needed.

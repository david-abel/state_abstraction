The combinatorial explosion that plagues planning and \ac{RL} algorithms can be moderated using abstraction. For instance, prohibitively difficult task representations can be condensed, preserving essential information such that solutions are tractably computable. In this work, we investigate a theoretical framework for approximate state abstraction that achieves high degrees of compression while preserving near optimal behavior. \ac{RL} agents using these abstractions treat experiences that resemble each other as equivalent, and generalize knowledge to novel scenarios based on prior experiences. We present theoretical guarantees of the quality of policies derived from four classes of approximate state abstraction. Additionally, we empirically evaluate the relationship between the degree of approximation and the degree of abstraction achieved in a variety of example tasks, as well as the tradeoff between the degree of approximation and the optimality of behavior.

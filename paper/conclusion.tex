\section{Conclusion}

In this work we investigate the aggregation of states on the basis of four classes of approximate criteria, and provide bounds for the value lost when behaving according to the optimal policy in the abstract \ac{MDP}. This approach is appealing for two reasons. First, approximate abstraction relies on criteria that we imagine a planning or learning algorithm to be able to learn without solving the full \ac{MDP}. Second, since the state-aggregation criteria are relaxed to approximate equality, methods that employ these techniques are able to tune the aggressiveness of state aggregation all the while incurring bounded error. We evaluate our approaches empirically on four different \acs{MDP}, providing visuals of the compression, as well as displaying the relationship between $\epsilon$ and the degree of compress, as well as the value of the abstract policy. We provide a code base that provides implementations to abstract, visualize, and evaluate an arbitrary MDP to promote further investigation into approximate abstraction.

There are many directions for future work. First, we are interested investigating learning approximate abstraction functions online in the planning or learning setting. \citeauthor{ortner2013adaptive} previously proposed this strategy by combining the approximate criteria of $\epsilon$-homogeneity with UCRL~\cite{ortner2007logarithmic}, and are curious how this strategy will fair under the proposed approximate abstraction families introduced in this paper. Learning these abstraction functions also poses interesting implications for the exploration problem. If certain information can inform the agent's ability to abstract more quickly, then exploration directed at learning the abstraction function introduces a new notion of exploration, since an abstraction function will enable the agent to identify near-optimal behavior more quickly, 

We are also interested in approximate relaxations of additional classes of abstraction functions, and in characterizing the necessary conditions for an abstraction that will preserve any notion of optimality. In particular, the abstraction class $\phi_{a^*}$ discussed by~\cite{li2006towards}, which is guaranteed to collapse states that share the optimal action and the optimal actions $Q^*$ value. To do this properly, one must apply confidence intervals to the event of each action's optimality, in addition to relaxing $Q^*$ equivalence to similarity. This abstraction class may also introduce compelling connections to temporal abstractions, including Options, discussed at length in reinforcement learning and planning \dnote{Options citations}. We investigated the use of options in the Taxi domain under $\phi_{a^*}$, and found a negligible difference in the degree and quality of compression compared to the compression without Options. In future work, we are interested in characterizing the relationship between temporal abstractions and approximate abstraction. Lastly, we are investigating the relationship between various approximate abstractions and the information theoretical limitations on the compressibility of certain classes of \acp{MDP}.


% Summary

% Future Work
\begin{enumerate}
\item Learning Phi
\begin{itemize}
\item Exploration vs. Exploitation problem is different while trying to learn Phi
\end{itemize}
\item Compressibility
\begin{itemize}
\item Relationship between approximate abstract and compressibility
\end{itemize}
\item POMDP and abstraction
\end{enumerate}


\section{Conclusion}

% Summary of contributions.
In this work we investigate the aggregation of states on the basis of four classes of approximate criteria, and provide bounds for the value lost when behaving according to the optimal policy in the abstract \ac{MDP}. This approach is appealing for two reasons. First, approximate abstraction relies on criteria that we imagine a planning or learning algorithm to be able to learn without solving the full \ac{MDP}. Second, since the state-aggregation criteria are relaxed to approximate equality, methods that employ these techniques are able to tune the aggressiveness of state aggregation all the while incurring bounded error. We evaluate our approaches empirically on four different \acs{MDP}, providing visuals of the compression, as well as displaying the relationship between $\epsilon$ and the degree of compress, as well as the value of the abstract policy. We provide a code base that provides implementations to abstract, visualize, and evaluate an arbitrary MDP to promote further investigation into approximate abstraction.

% Future work.
There are many directions for future work.

% Learning abs functions online.
First, we are interested investigating learning approximate abstraction functions online in the planning or learning setting, first proposed by \citeauthor{ortner2013adaptive} who learned the approximate criteria of $\epsilon$-homogeneity with UCRL~\cite{ortner2007logarithmic}.

% Exploration is interesting.
Learning abstraction functions online also has interesting implications for the exploration problem. If certain information can inform the agent's ability to abstract more quickly, then exploration directed at learning the abstraction function introduces a new notion of exploration, since an abstraction function will enable the agent to identify near-optimal behavior more quickly, 

% Necessary conditions.
One question of interest is what the necessary conditions are for an abstraction that will preserve any notion of optimality (i.e. the bound between the values under $\pi_G^*$, $\pi_{G,A}$ is less than $\textsc{VMax}$). In future work, we are interested in identifying these conditions.

% Options/termporal abstraction
In future work, we are interested in characterizing the relationship between temporal abstractions, such as those afforded by Options, and approximate abstractions. Options have been discussed at length in reinforcement learning and planning \dnote{Options citations}. We investigated the use of options in the Taxi domain, and found a negligible difference in the degree and quality of compression compared to compression without Options.  

% Compressibility
Lastly, we are investigating the relationship between various approximate abstractions and the information theoretical limitations on the compressibility of certain classes of \acp{MDP}.



@inproceedings{li2006towards,
  title={Towards a Unified Theory of State Abstraction for MDPs.},
  author={Li, Lihong and Walsh, Thomas J and Littman, Michael L},
  booktitle={ISAIM},
  year={2006}
}

@inproceedings{chentanez2004intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},
  booktitle={Advances in neural information processing systems},
  pages={1281--1288},
  year={2004}
}

@inproceedings{konidaris2007building,
  title={Building Portable Options: Skill Transfer in Reinforcement Learning.},
  author={Konidaris, George and Barto, Andrew G},
  booktitle={IJCAI},
  volume={7},
  pages={895--900},
  year={2007}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{kaelbling1996reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  pages={237--285},
  year={1996}
}

@inproceedings{ortner2007logarithmic,
  title={Logarithmic online regret bounds for undiscounted reinforcement learning},
  author={Ortner, P and Auer, R},
  booktitle={Proceedings of the 2006 Conference on Advances in Neural Information Processing Systems},
  volume={19},
  pages={49},
  year={2007}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT press}
}

@article{graphstream,
  author    = {Yoann Pign{\'{e}} and
               Antoine Dutot and
               Fr{\'{e}}d{\'{e}}ric Guinand and
               Damien Olivier},
  title     = {GraphStream: {A} Tool for bridging the gap between Complex Systems
               and Dynamic Graphs},
  journal   = {CoRR},
  volume    = {abs/0803.2093},
  year      = {2008},
  url       = {http://arxiv.org/abs/0803.2093},
}

@article{ortner2013adaptive,
  title={Adaptive aggregation for reinforcement learning in average reward Markov decision processes},
  author={Ortner, Ronald},
  journal={Annals of Operations Research},
  volume={208},
  number={1},
  pages={321--336},
  year={2013},
  publisher={Springer}
}

@article{strehl2009reinforcement,
  title={Reinforcement learning in finite MDPs: PAC analysis},
  author={Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  journal={The Journal of Machine Learning Research},
  volume={10},
  pages={2413--2444},
  year={2009},
  publisher={JMLR. org}
}

@inproceedings{littman1995complexity,
  title={On the complexity of solving Markov decision problems},
  author={Littman, Michael L and Dean, Thomas L and Kaelbling, Leslie Pack},
  booktitle={Proceedings of the Eleventh conference on Uncertainty in artificial intelligence},
  pages={394--402},
  year={1995},
  organization={Morgan Kaufmann Publishers Inc.}
}

@article{papadimitriou1987complexity,
  title={The complexity of Markov decision processes},
  author={Papadimitriou, Christos H and Tsitsiklis, John N},
  journal={Mathematics of operations research},
  volume={12},
  number={3},
  pages={441--450},
  year={1987},
  publisher={INFORMS}
}

@incollection{even2003approximate,
  title={Approximate equivalence of Markov decision processes},
  author={Even-Dar, Eyal and Mansour, Yishay},
  booktitle={Learning Theory and Kernel Machines},
  pages={581--594},
  year={2003},
  publisher={Springer}
}

@article{Jiang2015,
author = {Jiang, Nan},
title = {{Abstraction Selection in Model-Based Reinforcement Learning}},
volume = {37},
year = {2015}
}


@inproceedings{andre2002state,
  title={State abstraction for programmable reinforcement learning agents},
  author={Andre, David and Russell, Stuart J},
  booktitle={AAAI/IAAI},
  pages={119--125},
  year={2002}
}

@article{ravindran2003smdp,
  title={SMDP homomorphisms: An algebraic approach to abstraction in semi markov decision processes},
  author={Ravindran, Balaraman},
  year={2003}
}

@inproceedings{jiang2015abstraction,
  title={Abstraction Selection in Model-based Reinforcement Learning},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder},
  booktitle={Proceedings of The 32nd International Conference on Machine Learning},
  pages={179--188},
  year={2015}
}

@inproceedings{jong2005state,
  title={State Abstraction Discovery from Irrelevant State Variables.},
  author={Jong, Nicholas K and Stone, Peter},
  booktitle={IJCAI},
  pages={752--757},
  year={2005},
}

@inproceedings{ortner2014selecting,
  title={Selecting near-optimal approximate state representations in reinforcement learning},
  author={Ortner, Ronald and Maillard, Odalric-Ambrym and Ryabko, Daniil},
  booktitle={Algorithmic Learning Theory},
  pages={140--154},
  year={2014},
  organization={Springer}
}

@inproceedings{maillard2011selecting,
  title={Selecting the state-representation in reinforcement learning},
  author={Maillard, Odalric-Ambrym and Ryabko, Daniil and Munos, R{\'e}mi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2627--2635},
  year={2011}
}

@article{Bean2011,
author = {Bean, James C and Birge, John R and Smith, Robert L},
journal = {Operation research},
number = {2},
pages = {215--220},
title = {{Dynamic Programming Aggregation}},
volume = {35},
year = {2011}
}

@inproceedings{odalric2013optimal,
  title={Optimal regret bounds for selecting the state representation in reinforcement learning},
  author={Odalric-Ambrym, Maillard and Nguyen, Phuong and Ortner, Ronald and Ryabko, Daniil},
  booktitle={Proceedings of the Thirtieth International Conference on Machine Learning},
  year={2013}
}

@inproceedings{auer2009near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  booktitle={Advances in neural information processing systems},
  pages={89--96},
  year={2009}
}

@article{dietterich2000hierarchical,
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={J. Artif. Intell. Res.(JAIR)},
  volume={13},
  pages={227--303},
  year={2000}
}

@article{Boutilier2000,
author = {Boutilier, C and Dearden, Richard and Goldszmidt, M},
issn = {0004-3702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {49--107},
title = {{Stochastic dynamic programming with factored representations}},
volume = {121},
year = {2000}
}

@inproceedings{dean1997model,
  title={Model reduction techniques for computing approximately optimal solutions for Markov decision processes},
  author={Dean, Thomas and Givan, Robert and Leach, Sonia},
  booktitle={Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence},
  pages={124--131},
  year={1997},
  organization={Morgan Kaufmann Publishers Inc.}
}

@article{dearden1997abstraction,
  title={Abstraction and approximate decision-theoretic planning},
  author={Dearden, Richard and Boutilier, Craig},
  journal={Artificial Intelligence},
  volume={89},
  number={1},
  pages={219--283},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{dean1997modelmin,
  title={Model minimization in Markov decision processes},
  author={Dean, Thomas and Givan, Robert},
  booktitle={AAAI/IAAI},
  pages={106--111},
  year={1997}
}

@article{russell1995modern,
  title={A modern approach},
  author={Russell, Stuart and Norvig, Peter and Intelligence, Artificial},
  journal={Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs},
  volume={25},
  pages={27},
  year={1995},
  publisher={Citeseer}
}
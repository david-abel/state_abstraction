% --- SECTION: State Abstraction ---
\section{Approximate State Abstraction}

\label{sec:theory_results}
% Intro to approx state abstraction results.
Here, we introduce our formal analysis of approximate state abstraction, including results bounding the error associated with these abstraction methods. In particular, we demonstrate that abstractions based on approximate $Q^*$ similarity (\ref{sec:Q*}), approximate model similarity (\ref{sec:model}), and approximate similarity between distributions over $Q^*$, for both Boltzmann (\ref{sec:boltz}) and multinomial (\ref{sec:mult}) distributions, induce abstract \acp{MDP} for which the optimal policy has bounded error in the ground MDP.

We first introduce some additional notation.

\bdefn{$\pi_A^*$, $\pi_G^*$}
We let $\pi_A^* : \mcS_A \rightarrow \mcA$ and $\pi_G^* : \mcS_G \rightarrow \mcA$ stand for the optimal policies in the abstract and ground \acp{MDP}, respectively.
\edefn

% Abstract policy in ground.
We are interested in how the optimal policy in the abstract \ac{MDP} performs in the ground \ac{MDP}. As such, we formally define the policy in the ground \ac{MDP} derived from optimal behavior in the abstract \ac{MDP}:

\bdefn{$\pi_{GA}$}
Given a state $s \in \mcS_G$ and a state aggregation function, $\phi$, 
\begin{equation}
\pi_{GA}(s)=\pi_A^*(\phi(s)).
\end{equation}
\edefn

We now define classes of abstraction based on functions of state--action pairs.
\bdefn{$\ep_f$}
Given a function $f: \mathcal{S}_G \times \mathcal{A} \rightarrow \mathbb{R}$ and a fixed non-negative $\varepsilon \in \mathbb{R}$, we define $\ep_f$ as a type of approximate state-aggregation function that satisfies the following for any two ground states $s_1$ and $s_2$: 
\begin{equation}
\label{eq:phi_f}
\ep_f(s_1) = \ep_f(s_2) \rightarrow \forall_a \left|f(s_1, a) - f(s_2, a)\right| \leq \varepsilon.
\end{equation}
\edefn

That is, when $\ep_f$ aggregates states, all aggregated states have values of $f$ within $\varepsilon$ of each other for all actions.

\bdefn{$Q_G$, $V_G$}
Let $Q_G = Q^{\pi_G^*} : \mathcal{S}_G \times \mathcal{A} \rightarrow \mathbb{R} $ and $V_G = V^{\pi_G^*}: \mathcal{S}_G \rightarrow \mathbb{R} $ denote the optimal Q and optimal value functions in the ground \ac{MDP}.
\edefn

\bdefn{$Q_A$, $V_A$}
Let $Q_A  = Q^{\pi_A^*}: \mathcal{S}_A \times \mathcal{A} \rightarrow \mathbb{R}$ and $V_A  = V^{\pi_A^*}: \mathcal{S}_A \rightarrow \mathbb{R}$  stand for the optimal Q and optimal value functions in the abstract \ac{MDP}.
\edefn

We now introduce the main result of the paper.

% Theorem 1
\begin{thm}
There exist at least four classes of approximate state-aggregation functions, $\epQ$, $\epM$, $\epB$ and $\epMu$, for which the optimal policy in the abstract \ac{MDP}, applied to the ground \ac{MDP}, has suboptimality bounded polynomially in $\frac{1}{\varepsilon}$:
\begin{equation}
\forall_{s \in \mathcal{S}_G} V_G^{\pi_G^*}(s) - V_G^{\pi_{GA}}(s) \leq poly \left (\frac{1}{\varepsilon} \right).
\end{equation}
\end{thm}

% Subsection: Optimal Q Function
\subsection{Optimal Q Function: $\ep_{Q^*}$}
\label{sec:Q*}

% Q* Lemma -- lemma 1
We consider an approximate version of \citet{li2006towards}'s $\phi_Q^*$,. In our abstraction, states are aggregated together when their optimal $Q$-values are within $\varepsilon$.

\bdefn{$\epQ$}
An approximate $Q$ function has the same form as Equation~\ref{eq:phi_f}:
\begin{equation}
\ep_{Q^*}(s_1) = \ep_{Q^*}(s_2) \rightarrow \forall_a \left|Q_G^*(s_1, a) - Q_G^*(s_2, a)\right| \leq \varepsilon.
\end{equation}
\edefn

% Q^* Lemma
\begin{lma}
\label{lma:Q*}
When $\ep_{Q^*}$ is used to create the abstract state space $\mcS_A$:
\begin{equation}
\forall_{s \in \mcS_G} V_G^{\pi_G^*}(s) - V_G^{\pi_{GA}}(s) \leq \frac{2\varepsilon}{(1-\gamma)^2}.
\end{equation}
\end{lma}

% Proof
\textbf{Proof of Lemma~\ref{lma:Q*}:}
We first demonstrate that $Q$-values in the abstract \ac{MDP} are close to $Q$-values in the ground \ac{MDP} (Claim \ref{clm:closeQs}). We next leverage Claim \ref{clm:closeQs} to demonstrate that the optimal action in the abstract \ac{MDP} is nearly optimal in the ground \ac{MDP}  (Claim \ref{clm:optAbsActionNearOptGround}). Lastly, we use Claim \ref{clm:optAbsActionNearOptGround} to conclude Lemma \ref{lma:Q*} (Claim \ref{clm:lmaFromClm}).
% Claim 1:
\begin{clm}
\label{clm:closeQs}
Optimal $Q$-values in the abstract \ac{MDP} closely resemble optimal $Q$-values in the ground \ac{MDP}:
\begin{equation}
\label{eq:Q*Claim1}
\forall_{s_G \in \mathcal{S}_G, a} |Q_G(s_G, a) - Q_A(\epQ(s_G), a)| \leq \frac{\varepsilon}{1-\gamma}.
\end{equation}
\end{clm}

Consider a temporally heterogeneous \ac{MDP}, $M^T = \langle \mathcal{S}_T, \mathcal{A}_G, \mathcal{R}_T, \mathcal{T}_T, \gamma \rangle$, parameterized by integer $T$, such that for the first $T$ time steps the reward function, transition dynamics and state space are those of the abstract MDP, $M_A$, and after $T$ time steps the reward function, transition dynamics and state spaces are those of $M_G$. Thus,
\begin{align*}
\mathcal{S}_T &= \begin{cases}
\mathcal{S}_G,& \text{if } T \leq 0, \\
\mathcal{S}_A,& \text{o/w}
\end{cases}\\
%--
\mathcal{R}_T(s,a) &= \begin{cases}
\mathcal{R}_G(s,a),& \text{if } T \leq 0, \\
\mathcal{R}_A(s, a),& \text{o/w}
\end{cases}\\
%--
\mcT_T(s,a,s') &= \begin{cases}
\mcT_G(s,a,s'),& \text{if }T \leq 0, \\
\underset{{g \in G(s)}}{\sum}\left[\omega(g)\mcT_G(g, a, s')\right],& \text{if } T = 1, \\
\mcT_A(s,a,s'),& \text{o/w}
\end{cases}\\
\end{align*}

The $Q$-value of a state $s$ in $M^T$ for action $a$ is:
\begin{equation}
Q_T(s, a) = 
\begin{cases}
	   Q_G(s, a), &  \text{if } T=0,\\
	   \underset{g \in G(s)}{\sum} \left[ \omega(g)Q_G(g,a) \right], & \text{if } T = 1,\\
	   \mathcal{R}_A(\epQ(s),a) + \sigma_{T-1}(s,a), &\text{o/w},
\end{cases}
\end{equation}
where
\begin{equation*}
\sigma_{T-1}(s,a) = \gamma \underset{{s_A}' \in \mathcal{S}_A}{\sum} \mathcal{T}_A(\epQ(s),a,{s_A}') \max_{a'} Q_{T-1}({s_A}', a').
\end{equation*}

We proceed by induction on $T$ to show that:
\begin{equation}
\label{eq:clm1Induct}
\forall_{T, s_G \in \mathcal{S}_G, a} |Q_G(s_G, a) - Q_T(s_G, a)| \leq \sum_{t=0}^{T-1} \varepsilon \gamma^{t},
\end{equation}

% Base Case t=0
\textit{Base Case: $T = 0$}

When $T = 0$, $Q_T = Q_G$, so the result trivially follows.

% Base Case t=1
\textit{Base Case: $T = 1$}
\begin{align*}
&Q_1(s,a) = \underset{g \in G(s)}{\sum} \left[ \omega(g)Q_G(g,a) \right].
\end{align*}
Since all co-aggregated states have $Q$-values within $\varepsilon$ of one another and $\omega(g)$ induces a convex combination,
\begin{align*}
&Q_1(s,a) \leq \varepsilon \gamma^t + \varepsilon + Q_G(s_G, a).
\end{align*}
Thus,
\begin{equation}
\left| Q_{1}(s_A, a) - Q_G(s_G,a) \right| \leq \sum_{t=0}^{1}\varepsilon \gamma^t.
\end{equation}
% Inductive Case
\textit{Inductive Case: $T > 1$}

We assume as our inductive hypothesis that:
\begin{equation*}
\forall_{s_G \in \mathcal{S}_G, a} |Q_G(s_G, a) - Q_{T-1}(\epQ(s_G), a)| \leq \sum_{t=0}^{T-2} \varepsilon \gamma^t.
\end{equation*}

Consider a fixed but arbitrary state, $s_G \in \mathcal{S}_G$, and fixed but arbitrary action $a$.
We denote $\epQ(s_G)$ as $s_A$. 
By definition of $Q_{T}(s_A, a)$, $\mathcal{R}_A$, $\mathcal{T}_A$:
\begin{multline*}
Q_T(s_A, a) = \sum_{g \in G(s_A)}\omega(g)\ \times \\ 
 \left[ R_0(g,a) + \gamma \sum_{g' \in \mathcal{S}_G} T_0(g,a,g') \max_{a'} Q_{T-1}(g', a')      \right].
\end{multline*}
Applying our inductive hypothesis yields:
\begin{multline*}
Q_T(s_A, a) \leq \sum_{g \in G(s_A)}\omega(g) \times \biggl[ R_G(g,a)\ + \\ \gamma \sum_{g' \in \mathcal{S}_G} T_G(g,a,g') \max_{a'}(Q_G(g', a') + \sum_{t=0}^{T-2} \varepsilon \gamma^t) \biggr].
\end{multline*}
%&\leq \gamma\sum_{t=0}^{T-2} \varepsilon \gamma^t + \sum_{g \in X(s_A)}\omega(g)\left[ R_0(g,a) + \gamma \sum_{g' \in \mathcal{S}_G} T_0(g,a,g') \max_{a'}Q_0      \right]\\
Then,
\begin{equation*}
Q_T(s_A, a) \leq \gamma\sum_{t=0}^{T-2} \varepsilon \gamma^t + \sum_{g \in G(s_A)}\left[ \omega(g)\ Q_G(g,a)\right].
\end{equation*}
since all aggregated states have $Q$-values within $\varepsilon$ of one another:
\begin{align*}
Q_T(s_A, a) \leq \gamma\sum_{t=0}^{T-2} \varepsilon \gamma^t + \varepsilon + Q_G(s_G, a). \\
\therefore\ \left| Q_{T}(s_A, a) - Q_G(s_G,a) \right| \leq \gamma\sum_{t=0}^{T-1}\varepsilon \gamma^t
\end{align*}
Since $s_G$ and $s_A$ are arbitrary, we conclude Equation \ref{eq:clm1Induct}.

As $T \rightarrow \infty$, $\sum_{t=0}^{T-1} \varepsilon \gamma^t \rightarrow \frac{\varepsilon}{1-\gamma}$ by the sum of infinite geometric series and $Q_T \rightarrow Q_A$, Equation \ref{eq:clm1Induct} allows us to conclude Claim \ref{clm:closeQs}.

%Claim 2
\begin{clm}
\label{clm:optAbsActionNearOptGround}
%The optimal action in the abstract MDP has a $Q$-value in the ground which is nearly optimal:

Consider a fixed but arbitrary state, $s_G \in \mathcal{S}_G$ and its corresponding abstract state $s_A=\epQ(s_G)$.
Let $a^*_G$ stand for the optimal action in $s_G$, and $a^*_A$ stand for the optimal action in $s_A$:
\begin{align*}
a^*_G = \argmin Q_G(s_G, a), \hspace{4mm}
a^*_A = \argmin Q_A(s_A, a).
\end{align*}
The optimal action in the abstract MDP has a $Q$-value in the ground MDP that is nearly optimal:
\begin{equation}
\label{eq:Q*Claim2}
V_G(s_G) \leq Q_G(s_G, a^*_A) + \frac{2\varepsilon}{1-\gamma}.
\end{equation}
\end{clm}

By Claim~\ref{clm:closeQs},
\begin{align}
&V_G(s_G) = Q_G(s_G, a^*_G) \leq Q_A(s_A, a^*_G) + \frac{\varepsilon}{1-\gamma}.
\label{eq:Q*OptActionResult}
\end{align}

By the definition of $a^*_A$, we know that 
\begin{align}
Q_A(s_A, a^*_G) + \frac{\varepsilon}{1-\gamma} \leq Q_A(s_A, a^*_A) + \frac{\varepsilon}{1-\gamma}.
\end{align}

Lastly, again by Claim~\ref{clm:closeQs}, we know
\begin{align}
Q_A(s_A, a^*_A) + \frac{\varepsilon}{1-\gamma} \leq Q_G(s_g, a^*_A) + \frac{2\varepsilon}{1-\gamma}.
\end{align}
Therefore, Equation~\ref{eq:Q*Claim2} follows.

%Claim 3
\begin{clm}
Lemma \ref{lma:Q*} follows from Claim \ref{clm:optAbsActionNearOptGround}.
\label{clm:lmaFromClm}
\end{clm}

Consider the policy for $M_G$ of following the optimal abstract policy $\pi^*_A$ for $t$ steps and then following the optimal ground policy $\pi^*_G$ in $M_G$:
\begin{equation}
\pi_{A,t}(s)=
\begin{cases}
\pi_G^*(s), &\text{if } t<=0,\\
\pi_{GA}(s), &\text{if } t > 0.
\end{cases}
\end{equation}

For a particular $t$, the value of this policy for $s_G \in \mathcal{S}_G$ in the ground \ac{MDP} is as follows:
\begin{multline*}
V_G^{\pi_{A,t}}(s_G) = \\
R_G(s, \pi_{A,t}(s_G)) +\ \gamma \sum_{{s_G}' \in \mathcal{S}_G}\mathcal{T}_G(s_G, a, {s_G}')V_G^{\pi_{A,t-1}}({s_G}').
\end{multline*}
%Start induction on following the optimal abstract policy
We now show by induction on $t$ that
\begin{equation}
\forall_{t, s_G \in \mathcal{S}_g} V_G(s_G) \leq  V_G^{\pi_{A,t}}(s_G) + \sum_{i=0}^{t}\gamma^i \frac{2\varepsilon}{1-\gamma}.
\end{equation}

\textit{Base case: $t=0$}

By definition, when $t=0$, $V_G^{\pi_{A,t}} = V_G$, so our bound trivially holds in this case.

\textit{Inductive case: $t > 0$}

Consider a fixed but arbitrary state $s_G \in \mathcal{S}_G$.
We assume for our inductive hypothesis that
\begin{equation}
V_G(s_G) \leq V_G^{\pi_{A,t-1}}(s_G) + \sum_{i=0}^{t-1}\gamma^i \frac{2\varepsilon}{1-\gamma}.
\end{equation}
By definition,
\begin{multline*}
V_G^{\pi_{A,t}}(s_G) = R_G(s, \pi_{A,t}(s_G)) + \\ \gamma \sum_{g'}\mathcal{T}_G(s_G, a, {s_G}')V_G^{\pi_{A,t-1}}({s_G}').
\end{multline*}
Applying our inductive hypothesis yields:
\begin{multline*}
V_G^{\pi_{A,t}}(s_G) \geq R_G(s_G, \pi_{A,t}(s_G))\ + \\ \gamma \sum_{{s_G}'}\mathcal{T}_G(s_G, \pi_{A,t}(s_G), {s_G}')\left(V_G({s_G}') - \sum_{i=0}^{t-1}\gamma^i \frac{2\varepsilon}{1-\gamma} \right).
\end{multline*}
Therefore,
\begin{align*}
%&\geq -\gamma\sum_{i=0}^{t-1}\gamma^i \frac{2\varepsilon}{1-\gamma} + R_G(s, \pi_{A,t}(g)) + \gamma \sum_{g'}\mathcal{T}_G(g, a, g')V^{\pi^*}(g')\\
V_G^{\pi_{A,t}}(s_G) &\geq -\gamma\sum_{i=0}^{t-1}\gamma^i \frac{2\varepsilon}{1-\gamma} + Q_G(s_G, \pi_{A,t} (s_G)).
\end{align*}
Applying Claim~\ref{clm:optAbsActionNearOptGround} yields:
\begin{align*}
&V_G^{\pi_{A,t}}(s_G) \geq -\gamma\sum_{i=0}^{t-1}\gamma^i \frac{2\varepsilon}{1-\gamma} - \frac{2\varepsilon}{1-\gamma} + V_{G}(s_G) \\
%&\leq \sum_{i=0}^{t}\gamma^i \frac{2\varepsilon}{1-\gamma} + V_G(s_G)
\therefore\ &V_G(s_G) \leq V_G^{\pi_{A,t}}(s_G)  + \sum_{i=0}^{t}\gamma^i \frac{2\varepsilon}{1-\gamma}.
\end{align*}
Since $s_G$ was arbitrary, we conclude that our bound holds for all states in $\mathcal{S}_G$ for the inductive case.
Thus, from our base case and induction, we conclude that
\begin{equation}
\forall_{t, s_G \in \mathcal{S}_g} V_G^{\pi_G^*}(s_G) \leq  V_G^{\pi_{A,t}}(s_G) + \sum_{i=0}^{t}\gamma^i \frac{2\varepsilon}{1-\gamma}.
\end{equation}

Note that as $t \rightarrow \infty$, $\sum_{i=0}^{t}\gamma^i \frac{2\varepsilon}{1-\gamma} \rightarrow \frac{2\varepsilon}{(1-\gamma)^2}$ by the sum of infinite geometric series and $\pi_{A,t}(s) \rightarrow \pi_{GA}$.
Thus, we conclude Lemma~\ref{lma:Q*}.
\qed
%\begin{equation*}
%\forall_{s_G \in \mathcal{S}_g} V_G(s_G) \leq  V_G^{\pi_{GA}}(s_G) + \frac{2\varepsilon}{(1-\gamma)^2}
%\end{equation*}
%\begin{equation*}
%\forall_{s_G \in \mathcal{S}_g} \left | V_G(s_G) - V_G^{\pi_{GA}}(s_G) \right | \leq  \frac{2\varepsilon}{(1-\gamma)^2}
%\end{equation*}

% Subsection: Model Similarity -- lemma 2
\subsection{Model Similarity: $\ep_{model}$}
\label{sec:model}

Now, consider an approximate version of \citet{li2006towards}'s $\phi_{model}$, where states are aggregated together when their rewards and transitions are within $\varepsilon$.
\bdefn{$\epM$}
We let $\epM$ define a type of abstraction that, for fixed $\varepsilon$, satisfies:
\begin{multline}
\epM(s_1) = \epM(s_2) \rightarrow \\
\forall_a \left| \mcR_G(s_1, a) - \mcR_G(s_2, a)\right| \leq \varepsilon\; \text{~AND}\ \\
\forall_{s_A \in \mcS_A} \left|\sum_{{s_G}' \in G(s_A)} \left[\mcT_G(s_1, a, {s_G}') - \mcT_G(s_2, a,{s_G}')\right] \right| \leq \varepsilon.
\end{multline}
\edefn

%Model Lemma
\begin{lma}
\label{lma:model}
When $\mcS_A$ is created using a function of the $\ep_{model}$ type:
\begin{equation}
\forall_{s \in \mcS_G} V_G^{\pi_G^*}(s) - V_G^{\pi_{GA}}(s) \leq \frac{2\varepsilon + 2\gamma((|\mcS_G|-1)\varepsilon)}{(1-\gamma)^3}.
\end{equation}
\end{lma}

{\bf Proof of Lemma~\ref{lma:model}:}

Let $B$ be the maximum $Q$-value difference between any pair of ground states in the same abstract state for $\epM$:
\begin{equation*}
B = \max_{s_A, s_1, s_2, a}  |Q_G(s_1, a) - Q_G(s_2, a)|,
\end{equation*}
where $s_A \in \mcS_G$ and $s_1, s_2 \in G(s_A)$. Since difference of rewards is bounded by $\varepsilon$:
% Removed for space
%\begin{multline*}
%B=\max_{s_A, s_1, s_2, a}      \biggl|\mcR_G(s_1, a) - \mcR_G(s_2, a)\ + \gamma \sum_{{s_G}' \in \mcS_G} \\
%\biggl[(\mcT_G(s_1,a,{s_G}') -\mcT_G(s_2, a, {s_G}'))\max_{a'}Q_G({s_G}', a')\biggr]\biggr|
%\end{multline*}
\begin{multline*}
B\leq \varepsilon + \gamma \sum_{s_A \in \mcS_A}\sum_{{s_G}' \in G(s_A)} \biggl[(T_G(s_1, a, {s_G}')\ -\\ 
T_G(s_2, a, {s_G}')) \max_{a'}Q_G({s_G}', a') \biggr].
\end{multline*}
By similarity of transitions under $\epM$:
\begin{align*}
B \leq \varepsilon + \gamma \textsc{QMax} \sum_{s_A \in \mcS_A} \varepsilon \leq \varepsilon + \gamma|\mcS_G|\varepsilon \textsc{QMax}.
\end{align*}
Since \textsc{QMax} $\leq \frac{\textsc{RMax}}{1-\gamma}$, and we defined $\textsc{RMax} = 1$:
\begin{equation*}
B \leq \frac{\varepsilon + \gamma(|\mcS_G| - 1) \varepsilon}{1-\gamma}.
\end{equation*}
Since the $Q$-values of ground states grouped under $\epM$ are strictly less than $B$, we can understand $\epM$ as a type of $\epQ$ with $\varepsilon = B$. Applying Lemma \ref{lma:Q*} yields Lemma \ref{lma:model}.
\qed

\subsection{Boltzmann over Optimal Q: $\epB$}
\label{sec:boltz}

%zzz all this theory is quite abstract. is there some way to weave in a running example and/or to at least provide some more motiviation for why phiBoltz is desirable. Balancing exploration/exploitation isn't really a motivation for why values should be abstracted together. What do the first two abstraction types miss that this one provides? We didn't Lihong et al. study it in the exact case?
Here, we introduce $\epB$, which aggregates states with similar Boltzmann distributions on $Q$-values. This family of abstractions is appealing as Boltzmman distributions balance exploration and exploitation~\cite{sutton1998reinforcement}. We find this family particularly interesting for abstraction purposes as, unlike $\epQ$, it allows for aggregation when $Q$-value ratios are similar but their magnitudes are different.

\bdefn{$\epB$}
We let $\epB$ define a family of abstractions that, for fixed $\varepsilon$, satisfies:
\begin{multline}
\epB(s_1) = \epB(s_2) \rightarrow \\
\forall_{a} \left|\frac{e^{Q_G(s_1,a)}}{\sum_b e^{Q_G(s_1,b)}} - \frac{e^{Q_G(s_2,a)}}{\sum_b e^{Q_G(s_2,b)}}\right| \leq \varepsilon.
\label{eq:phi_bolt}
\end{multline}
\edefn

We also assume that the difference in normalizing terms is bounded by some non-negative constant, $k\in \mathbb{R}$, of $\varepsilon$:
\begin{equation}
\left| \sum_b e^{Q_G(s_1,b)} - \sum_b e^{Q_G(s_2,b)} \right| \leq k\times\varepsilon.
\label{eq:bolt_denom}
\end{equation}
\begin{lma} When $S_A$ is created using a function of the $\epB$ type, for some non-negative constant $k \in \mathbb{R}$:
\begin{equation}
\forall_{s \in \mcS_G} V_G^{\pi^*_G}(s) - V_G^{\pi_{GA}}(s) \leq \frac{2\varepsilon\left(\frac{|\mathcal{A}|}{1-\gamma} + k\varepsilon + k\right)}{(1-\gamma)^2}.
\end{equation}
\label{lma:bolt_lemma}
\end{lma}
We use the approximation for $e^x$, with $\delta$ error:
\begin{equation}
 e^x = 1 + x + \delta  \approx 1 + x.
\label{eq:e_to_x_approx}
\end{equation}

{\bf Proof Sketch of Lemma~\ref{lma:bolt_lemma}:}

By the approximation in Equation~\ref{eq:e_to_x_approx} and the assumption in Equation~\ref{eq:bolt_denom}:
\begin{align*}
\left|\frac{1 + Q_G(s_1,a) + \delta_1}{\sum_j e^{Q_G(s_1,a_j)}} - \frac{1 + Q_G(s_2,a) + \delta_2}{\sum_j e^{Q_G(s_1,a_j)} \pm k\varepsilon}\right| \leq \varepsilon
\end{align*}
It follows that:
\begin{equation}
\left|Q_G(s_1,a) - Q_G(s_2,a)\right| \leq \varepsilon \left(\frac{|\mathcal{A}|}{1-\gamma} + k\varepsilon + k \right).
\label{eq:bolt_qs}
\end{equation}
Consequently, we can consider $\epB$ as a variation of $\epQ$, where the difference in $Q$-values is bounded by $\varepsilon \left(\frac{|\mathcal{A}|}{1-\gamma} + k\varepsilon + k \right)$. Lemma~\ref{lma:bolt_lemma} then follows directly from Lemma~\ref{lma:Q*}.
\qed

% Subsection: Multinomial over Optimal Q
\subsection{Multinomial over Optimal Q: $\epMu$}
\label{sec:mult}

We consider approximate abstractions derived from a multinomial distribution over $Q^*$ for similar reasons to the Boltzmann distribution. Additionally, the multinomial distribution is appealing for its simplicity.
\bdefn{$\epMu$}
We let $\epMu$ define a type of abstraction that, for fixed $\varepsilon$, satisfies
\begin{multline}
\epMu(s_1) = \epMu(s_2) \rightarrow \\
\forall_{a} \left|\frac{Q_G(s_1,a)}{\sum_b Q_G(s_1,b)} - \frac{Q_G(s_1,a)}{\sum_b Q_G(s_1,b)}\right| \leq \varepsilon.
\end{multline}
\edefn

We also assume that the difference in normalizing terms is bounded by some non-negative constant, $k \in \mathbb{R}$, of $\varepsilon$:
\begin{equation}
\left |\sum_i Q_G(s_1,a_i) - \sum_j Q_G(s_2,a_j) \right | \leq k\varepsilon.
\end{equation}
\begin{lma} When $S_A$ is created using a function of the $\epMu$ type, for some non-negative constant $k \in \mathbb{R}$:
\begin{equation}
\forall_{s \in S_M} V_G^{\pi^*_G}(s) - V_G^{\pi_{GA}}(s) \leq \frac{\frac{2\varepsilon|\mathcal{A}|}{1-\gamma} + k \varepsilon^2 + k}{(1-\gamma)^2}.
\end{equation}
\label{lma:mult_lemma}
\end{lma}

{\bf Proof Sketch of Lemma~\ref{lma:mult_lemma}} The proof follows an identical strategy to that of Lemma~\ref{lma:bolt_lemma}, but without $e^x \approx 1+x$.
\qed


% Subsection: Other Abstractions
%\subsection{Other Abstractions}
%
%We note that one natural way of approximating $\phi_{a^*}$ from , in which states that are compressed together share optimal actions and the Q values of these actions are within $\varepsilon$ is ultimately equivalent to the crisp abstraction $\phi_{\pi^*}$, in states that are compressed share an optimal action. A true approximation of $\phi_{a^*}$ ought to also approximate the optimality of each action. Given the degree of compression achievable under $\phi_{a^*}$, especially with temporally extended actions, we foresee this approximate form of abstraction as being of great interest, and plan to investigate it in future work.
%
%We are {\it not} going to provide results for $\phi_{\pi^*}$, since relaxing equality of optimal actions doesn't mean anything.
%
%Additionally, we are {\it not} going to provide results for $\phi_{Q^\pi}$, since as Lihong's paper notes, ``It is an open question how to find $Q^\pi$ irrelevance abstractions without enumerating all possible policies, they do not give results. Furthermore, an MDP for which this is true is an awfully weird MDP...
%
%Lastly, we are interested in a generalization of $\phi_{mult}$  and $\phi_{bolt}$ that handles a broader space of distributions over Q values.
%
%We also note that any abstraction that depends only on the reward function, $\mathcal{R}$, can incur unbounded error (or rather, $\textsc{VMax}$).`

% Some abstractions DON'T preserve a meaningful notion of optimality.



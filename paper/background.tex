\section{Background}

First, some background.\dnote{Mmm. Hmmm? mmmm.}

%MDP/SDM Background and Notation
\subsection{\acp{MDP} and Sequential Decision Making}
An \ac{MDP} is a five-tuple: $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$: $\mathcal{S}$ is a finite state space; $\mathcal{A}$ is the set of actions available to the agent; $\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the probability of an agent applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$ resulting in arriving in $s' \in \mathcal{S}$; $\mathcal{R}(s)$ denotes the reward received by the agent arriving in state $s$; $\gamma \in [0, 1]$ is a discount  factor that defines how much the agent prefers future rewards over immediate rewards. We assume without loss of generality that all reward functions are normalized to $[0,1]$.

The goal of an agent in an \ac{MDP} is to solve for a policy that maps states to actions: $\pi: \mathcal{S} \rightarrow \mathcal{A}$. In particular, the agent wants to solve for the policy that maximizes its expected discounted reward from any state. This goal policy is denoted $\pi^*$. We denote the expected discounted reward for following policy $\pi$ from state $s$ as the value of the state under that policy, $V^\pi(s)$. We similarly denote the expected discounted reward for taking action $a \in \mathcal{A}$ and then following policy $\pi$ from state $s$ forever after as $Q^\pi(s,a)$. Lastly, we denote the value and $Q$ functions under the optimal policy as $V^*$ and $Q^*$, respectively. \enote{Add citation -- either Littman review paper or Sutton/Barto or Puterman}

\enote{Define value of state under a policy}
\dnote{Q function, since we use it a lot}

%Lihong Section
\subsection{Abstraction Notation}
We build upon the notation used by \citeauthor{li2006towards} \enote{This should probably say more}.

We understand an abstraction as a mapping from the state space of a ground MDP $M_G$ to that of an abstract MDP $M_A$ using a state-aggregation scheme. $M_G = \langle \mathcal{S}_G, \mathcal{A}, \mathcal{T}_G, \mathcal{R}_G, \gamma \rangle$ and $M_A = \langle \mathcal{S}_A, \mathcal{A}, \mathcal{T}_A, \mathcal{R}_A, \gamma \rangle$.

\dnote{Might be worth noting that $\phi$ induces a full abstract MDP in the ``intuitively" section}

% States
The states in the abstract \ac{MDP} are constructed by applying a state-aggregation function, $\phi$, to the states in the ground \ac{MDP}, $\mathcal{S}_A$. More specifically, $\phi$ maps a state in the ground \ac{MDP} to a state in the abstract \ac{MDP}:
\begin{equation}
\mathcal{S}_A = \{ \phi(s) \mid s \in \mathcal{S}_G\}.
\end{equation}

Intuitively, the abstract reward function and abstract transition dynamics for each abstract state are a weighted combination of the rewards and transitions for each ground state in the partition. We make no assumptions about the weighting scheme, except that the weighting is a convex combination across all states in the partition.

% Reward
The abstract reward function $\mathcal{R}_A(s,a)$ is a weighted sum of the rewards of each of the ground states that map to the same abstract state:
\begin{equation}
\mathcal{R}_A(s,a) = \sum_{g \in \phi(s), s \in \mathcal{S}_G} \mathcal{R}_G(g,a) \omega(g) .
\end{equation}
\dnote{Need to change the subscript here.}
% zzz g in phi(s) seems like a type error, no?

% Transition
The abstract transition function $\mathcal{T}_A(s,a,s')$ is a weighted sum of the transitions of each of the ground states that map to the same abstract state:
\begin{equation}
\mathcal{T}_A(s,a,s') = \sum_{g \in \phi^{-1}(s)} \sum_{g' \in \phi^{-1}(s')} \mathcal{T}_G(g,a,g') \omega(g),
\end{equation}
\noindent where $
s, s' \in \mathcal{S}_A
$.

% Abstract policy in ground.
In general, we are interested in how policies defined over the abstract \ac{MDP} perform in the ground \ac{MDP}. To evaluate a policy $\pi_A:\mathcal{A}\rightarrow\mathcal{A}$, we map a ground state into the abstract state space, and query the abstract policy for behavior. Specifically:
\begin{equation}
V^{\pi_A}(s) = \mathcal{R}_G(s,\pi_A(\phi(s))) + \gamma \sum_{s' \in \mathcal{S}_G} \mathcal{T}_G(s,\pi_A(\phi(s)),s')V^{\pi_A}(s').
\end{equation}

\enote{Define $V_G$ and $V_A$ (value function in ground and value function in abstract)}
\dnote{This should be done in the background section, right? Just defining a value fund under a policy}
\enote{Say more about the weighting, w(s), (that it sums to 1 but that's all we assume)}

% Subsection: Exact Abstraction Framework
\subsection{Exact Abstraction Framework}

\citeauthor*{li2006towards} developed a framework for state abstraction in \acp{MDP}, the notation for which is defined in the previous section. Here, we survey their main results, which we then extend to the framework of approximate abstraction.

In particular, the authors define the following five classes of state-aggregation functions, inspired by existing methods for state abstraction in \acp{MDP}. For brevity, we include those that we build upon directly. (See their paper for full details.)

\begin{defn}
A $Q^*$-irrelevance abstraction, $\phi_{Q^*}$, is one such that:
\begin{equation}
\phi_{Q^*}(s_1) = \phi_{Q^*}(s_2) \rightarrow \forall a\ \left(Q^*(s_1,a) = Q^*(s_2,a)\right).
\end{equation}
\end{defn}

\begin{defn}
A model-irrelevance abstraction, $\phi_{model}$, is one such that:
\begin{multline}
\phi_{model}(s_1) = \phi_{model}(s_2) \rightarrow \forall a\ \mathcal{R}_G(s_1,a) = \mathcal{R}_G(s_2,a). \\
\end{multline}
and
\begin{multline}
\phi_{model}(s_1) = \phi_{model}(s_2) \rightarrow \\ \forall a \sum_{s' \in \phi^{-1}(\phi(s_1))} \mathcal{T}_G(s_1,a,s') =\sum_{s' \in \phi^{-1}(\phi(s_2))} \mathcal{T}_G(s_2,a,s'). \\
\end{multline}
\end{defn}

In words, zzz.\dnote{Insert details about each $\phi$ above}

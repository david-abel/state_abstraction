
\section{Background}

%First, some background.

zzz: I think empty sections like this are ugly. There ought to be some sort of preamble here.

%MDP/SDM Background and Notation
\subsection{\acp{MDP} and Sequential Decision Making}
\enote{Come back to this and make sure it covers everything that needs to be covered and matches all our notation.}
An \ac{MDP} is a problem representation for sequential decision making agents, represented by a five-tuple: $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$. Here, $\mathcal{S}$ is a finite state space; $\mathcal{A}$ is a finite set of actions available to the agent; $\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the probability of an agent transitioning to state $s' \in \mathcal{S}$ after applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$; $\mathcal{R}(s,a)$ denotes the reward received by the agent for executing action $a$ in state $s$; $\gamma \in [0, 1]$ is a discount factor that determines how much the agent prefers future rewards over immediate rewards. We assume without loss of generality that the range of all reward functions is normalized to $[0,1]$. The solution to an \ac{MDP} is called a policy, denoted $\pi: \mathcal{S} \mapsto \mathcal{A}$.

The objective of an agent is to solve for the policy that maximizes its expected discounted reward from any state, denoted $\pi^*$. We denote the expected discounted reward for following policy $\pi$ from state $s$ as the value of the state under that policy, $V^\pi(s)$. We similarly denote the expected discounted reward for taking action $a \in \mathcal{A}$ and then following policy $\pi$ from state $s$ forever after as $Q^\pi(s,a)$, defined by the Bellman Equation as:
\begin{equation}
Q^\pi(s,a) = \mathcal{R}(s,a) + \gamma \sum_{s'} \mathcal{T}(s,a,s') Q^\pi(s',\pi(s)).
\end{equation}
Similarly, the value function defined under a given policy, denoted $V^\pi(s)$, is defined as:
\begin{equation}
V^\pi(s) = \max_a Q^\pi(s,a).
\end{equation}

zzz no, that is not the standard definition. it would normally be $V^\pi(s) = Q^\pi(s,\pi(s))$.

Lastly, we denote the value and $Q$ functions under the optimal policy as $V^*$ or $V^{\pi^*}$ and $Q^*$ or $Q^{\pi^*}$, respectively. For further background, see~\namecite{kaelbling1996reinforcement}.

%Lihong Section
\subsection{Abstraction Notation}
We build upon the notation used by \citeauthoryear{li2006towards}, who introduced a unifying theoretical framework for state abstraction in \acp{MDP} and sequential decision making. We build heavily on their notation, and discuss other similarities to our work in the next section.

\bdefn{$M_G$, $M_A$}
We understand an abstraction as a mapping from the state space of a ground MDP $M_G$ to that of an abstract MDP $M_A$ using a state-aggregation scheme. Consequently, this mapping induces an abstract MDP. Let $M_G = \langle \mathcal{S}_G, \mathcal{A}, \mathcal{T}_G, \mathcal{R}_G, \gamma \rangle$ and $M_A = \langle \mathcal{S}_A, \mathcal{A}, \mathcal{T}_A, \mathcal{R}_A, \gamma \rangle$.
\edefn

% States
\bdefn{$\mcS_A$, $\phi$}
The states in the abstract \ac{MDP} are constructed by applying a state-aggregation function, $\phi$, to the states in the ground \ac{MDP}, $\mathcal{S}_A$. More specifically, $\phi$ maps a state in the ground \ac{MDP} to a state in the abstract \ac{MDP}:
\begin{equation}
\mathcal{S}_A = \{ \phi(s) \mid s \in \mathcal{S}_G\}.
\end{equation}
\edefn

zzz the space before the comma looks weird to me.

\bdefn{$G$}
Given a $\phi$, each ground state has associated with it the ground states with which it is aggregated. Similarly, each abstract state has its constituent ground states. We let $G$ be the function that retrieves these states:
\begin{equation}
G(s)=
\begin{cases}
\{g \in \mcS_G \mid \phi(g) = \phi(s) \},& s \in \mcS_G,\\
\{g \in \mcS_G \mid \phi(g)=s \},& s \in \mcS_A.
\end{cases}
\end{equation}
\edefn

The abstract reward function and abstract transition dynamics for each abstract state are a weighted combination of the rewards and transitions for each ground state in the partition. We make no assumptions about the weighting scheme, except that for each abstract state, the weighting is a convex combination across all ground states.

zzz it's weird that s is bound twice in this formula. is that needed/intentional?

\bdefn{$\omega(s)$}
We refer to the weight associated with a ground state, $s \in \mathcal{S}_G$ by $\omega(s)$. The only restriction placed on the weighting scheme is that it induces a probability distribution on the states:
\begin{equation}
\forall s \in \mcS_G \left(\sum_{s \in G(s)} \omega(s)\right) = 1 \textrm{ and }\hspace{6mm}  \omega(s) \in [0,1].
\end{equation}
\edefn

% Reward
\bdefn{$\mcR_A$}
The abstract reward function $\mathcal{R}_A(s,a)$ is a weighted sum of the rewards of each of the ground states that map to the same abstract state:
\begin{equation}
\mathcal{R}_A(s,a) = \sum_{g \in G(s)} \mathcal{R}_G(g,a) \omega(g) .
\end{equation}
\edefn

% Transition
\bdefn{$\mcT_A$}
The abstract transition function $\mathcal{T}_A(s,a,s')$ is a weighted sum of the transitions of each of the ground states that map to the same abstract state:
\begin{equation}
\mathcal{T}_A(s,a,s') = \sum_{g \in G(s)} \sum_{g' \in G(s')} \mathcal{T}_G(g,a,g') \omega(g).
\end{equation}
\edefn



%\enote{Define $V_G$ and $V_A$ (value function in ground and value function in abstract)}
%\dnote{This should be done in the background section, right? Just defining a value fund under a policy}

% Subsection: Exact Abstraction Framework









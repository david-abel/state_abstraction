\section{\acp{MDP} and Sequential Decision Making}

%First, some background.

%zzz: I think empty sections like this are ugly. There ought to be some sort of preamble here.

%MDP/SDM Background and Notation
\enote{Come back to this and make sure it covers everything that needs to be covered and matches all our notation.}
An \ac{MDP} is a problem representation for sequential decision making agents, represented by a five-tuple: $\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma \rangle$. Here, $\mathcal{S}$ is a finite state space; $\mathcal{A}$ is a finite set of actions available to the agent; $\mathcal{T}$ denotes $\mathcal{T}(s' \mid s,a)$, the probability of an agent transitioning to state $s' \in \mathcal{S}$ after applying action $a \in \mathcal{A}$ in state $s \in \mathcal{S}$; $\mathcal{R}(s,a)$ denotes the reward received by the agent for executing action $a$ in state $s$; $\gamma \in [0, 1]$ is a discount factor that determines how much the agent prefers future rewards over immediate rewards. We assume without loss of generality that the range of all reward functions is normalized to $[0,1]$. The solution to an \ac{MDP} is called a policy, denoted $\pi: \mathcal{S} \mapsto \mathcal{A}$.

The objective of an agent is to solve for the policy that maximizes its expected discounted reward from any state, denoted $\pi^*$. We denote the expected discounted reward for following policy $\pi$ from state $s$ as the value of the state under that policy, $V^\pi(s)$. We similarly denote the expected discounted reward for taking action $a \in \mathcal{A}$ and then following policy $\pi$ from state $s$ forever after as $Q^\pi(s,a)$, defined by the Bellman Equation as:
\begin{equation}
Q^\pi(s,a) = \mathcal{R}(s,a) + \gamma \sum_{s'} \mathcal{T}(s,a,s') Q^\pi(s',\pi(s)).
\end{equation}
Similarly, the value function defined under a given policy, denoted $V^\pi(s)$, is defined as:
\begin{equation}
V^\pi(s) = Q^\pi(s,a).
\end{equation}

%zzz no, that is not the standard definition. it would normally be $V^\pi(s) = Q^\pi(s,\pi(s))$.

Lastly, we denote the value and $Q$ functions under the optimal policy as $V^*$ or $V^{\pi^*}$ and $Q^*$ or $Q^{\pi^*}$, respectively. For further background, see~\citep{kaelbling1996reinforcement}.

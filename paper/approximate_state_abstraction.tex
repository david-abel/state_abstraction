\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage{aaai}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\title{Approximate State Abstraction}
\author{David Abel, D. Ellis Hershkowitz, Michael Littman}
\date{}                                           % Activate to display a given date or no date

% --- Note Commands ---
\usepackage{color}
\newcommand\dnote[1]{\textcolor{blue}{Dave: #1}}
\newcommand\enote[1]{\textcolor{green}{Ellis: #1}}

\begin{document}
\maketitle


% --- ABSTRACT ---
\begin{abstract}
The combinatorial explosion that plagues planning and reinforcement learning algorithms can be reversed using abstraction. For instance, prohibitively difficult robotics task representations can be condensed so that solutions are tractably computable. In this work, we investigate a theoretical framework for approximate state abstraction that preserves near optimal behavior. Reinforcement learning agents using these abstractions may treat experiences that resemble each other as equivalent, and generalize knowledge to novel scenarios based on prior experiences. We present preliminary results and directions for future work.

Abstraction lies at the heart of computation. In this work, we demonstrate that the combinatorial explosion central to many issues in artificial intelligence can be reversed using effective abstraction methods. We extend the framework of state abstraction proposed by (Lihong et. al. 2005) to allow for approximately optimal state abstraction, and demonstrate that, in ideal scenarios, this framework allows for arbitrary reduction in the sample complexity of learning in certain learning problem. We develop methods for creating abstract problem representations and prove that their solutions have bounded error on the original, un-abstracted problem. Furthermore, we provide visualizations of the abstracted problems.

\end{abstract}



% --- SECTION: Introduction ---
\section{Introduction}

\begin{itemize}
\item Solving MDPs is P-Complete, where the dominant factor is $|\mathcal{S}|$.
\item Sample complexity is {\it also} dominated by $|\mathcal{S}|$.
\item So, in order to make MDPs with large $|\mathcal{S}|$ tractable, we can reduce them to a simpler form.
\item In this work we show that compressing MDPs in a particular way allows for the transfer of bounded-error solutions between the compressed MDP and ground MDP.
\item Q: Why approximate?
\item A: Approximate can compress strictly more than compression based on equality
\item A: Approximate compression requires the type of knowledge that we could imagine learning (i.e.. approximate knowledge)
\end{itemize}


% --- SECTION: Background ---
\section{Background}

% Quick MDP blurb

% Previous state aggregation stuff?


% --- SECTION: Related Work ---
\section{Related Work}

% Lihong

% Nan/Lihong

% Approximate bisimulation

% Other folks...


% --- SECTION: State Abstraction ---
\section{Approximate State Abstraction}

% Explain each of the Phi's
% Subsections for proofs for each phi
\dnote{Insert lemma for each phi as their subsection}


% Subsection: Phi_M
\subsection{$\Phi_M$}



% Subsection: Phi_Q
\subsection{$\Phi_Q^*$}



% Subsection: Phi_D
\subsection{$\Phi_D$}



% Subsection: Other Abstractions
\subsection{Other Abstractions}




% --- SECTION: Example Domains ---
\section{Example Domains}

\subsection{UpWorld}

\dnote{As part of upworld we can include the result about the unbounded improvement in sample complexity}

% Insert visual and/or stats on number of states and performance of VI solving the abstract MDP and evaluating the resulting policy on the original MDP



\subsection{NChain}

% Insert visual and/or stats on number of states and performance of VI solving the abstract MDP and evaluating the resulting policy on the original MDP



\subsection{Trench}

% Insert visual and/or stats on number of states and performance of VI solving the abstract MDP and evaluating the resulting policy on the original MDP




% --- SECTION: Conclusion ---
\section{Conclusion}

% Summary

% Future Work
\begin{enumerate}
\item Learning Phi
\begin{itemize}
\item Exploration vs. Exploitation problem is different while trying to learn Phi
\end{itemize}
\item Compressibility
\begin{itemize}
\item Relationship between approximate abstract and compressibility
\end{itemize}
\item 
\end{enumerate}





% --- BIBLIOGRAPHY ---
\bibliographystyle{aaai}
\bibliography{state_abs}

\end{document}
